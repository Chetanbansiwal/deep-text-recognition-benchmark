{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Samajh_AI_multi-output_model.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Et_5m9pgOYka",
        "-XBh50YGOZFr",
        "GYgz6JSVQKZm",
        "TVXiZZRkOZfw",
        "TONLfaK81DDx"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Transformation"
      ],
      "metadata": {
        "id": "VKlr5HpyOUV6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hp-Nz-8qRD_K"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "class TPS_SpatialTransformerNetwork(nn.Module):\n",
        "    \"\"\" Rectification Network of RARE, namely TPS based STN \"\"\"\n",
        "\n",
        "    def __init__(self, F, I_size, I_r_size, I_channel_num=3):                                                #changed for RGB colored image\n",
        "        \"\"\" Based on RARE TPS\n",
        "        input:\n",
        "            batch_I: Batch Input Image [batch_size x I_channel_num x I_height x I_width]\n",
        "            I_size : (height, width) of the input image I\n",
        "            I_r_size : (height, width) of the rectified image I_r\n",
        "            I_channel_num : the number of channels of the input image I\n",
        "        output:\n",
        "            batch_I_r: rectified image [batch_size x I_channel_num x I_r_height x I_r_width]\n",
        "        \"\"\"\n",
        "        super(TPS_SpatialTransformerNetwork, self).__init__()\n",
        "        self.F = F\n",
        "        self.I_size = I_size\n",
        "        self.I_r_size = I_r_size  # = (I_r_height, I_r_width)\n",
        "        self.I_channel_num = I_channel_num\n",
        "        self.LocalizationNetwork = LocalizationNetwork(self.F, self.I_channel_num)\n",
        "        self.GridGenerator = GridGenerator(self.F, self.I_r_size)\n",
        "\n",
        "    def forward(self, batch_I):\n",
        "        batch_C_prime = self.LocalizationNetwork(batch_I)  # batch_size x K x 2\n",
        "        build_P_prime = self.GridGenerator.build_P_prime(batch_C_prime)  # batch_size x n (= I_r_width x I_r_height) x 2\n",
        "        build_P_prime_reshape = build_P_prime.reshape([build_P_prime.size(0), self.I_r_size[0], self.I_r_size[1], 2])\n",
        "        \n",
        "        if torch.__version__ > \"1.2.0\":\n",
        "            batch_I_r = F.grid_sample(batch_I, build_P_prime_reshape, padding_mode='border', align_corners=True)\n",
        "        else:\n",
        "            batch_I_r = F.grid_sample(batch_I, build_P_prime_reshape, padding_mode='border')\n",
        "\n",
        "        return batch_I_r\n",
        "\n",
        "\n",
        "class LocalizationNetwork(nn.Module):\n",
        "    \"\"\" Localization Network of RARE, which predicts C' (K x 2) from I (I_width x I_height) \"\"\"\n",
        "\n",
        "    def __init__(self, F, I_channel_num):\n",
        "        super(LocalizationNetwork, self).__init__()\n",
        "        self.F = F\n",
        "        self.I_channel_num = I_channel_num\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=self.I_channel_num, out_channels=64, kernel_size=3, stride=1, padding=1,\n",
        "                      bias=False), nn.BatchNorm2d(64), nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2),  # batch_size x 64 x I_height/2 x I_width/2\n",
        "            nn.Conv2d(64, 128, 3, 1, 1, bias=False), nn.BatchNorm2d(128), nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2),  # batch_size x 128 x I_height/4 x I_width/4\n",
        "            nn.Conv2d(128, 256, 3, 1, 1, bias=False), nn.BatchNorm2d(256), nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2),  # batch_size x 256 x I_height/8 x I_width/8\n",
        "            nn.Conv2d(256, 512, 3, 1, 1, bias=False), nn.BatchNorm2d(512), nn.ReLU(True),\n",
        "            nn.AdaptiveAvgPool2d(1)  # batch_size x 512\n",
        "        )\n",
        "\n",
        "        self.localization_fc1 = nn.Sequential(nn.Linear(512, 256), nn.ReLU(True))\n",
        "        self.localization_fc2 = nn.Linear(256, self.F * 2)\n",
        "\n",
        "        # Init fc2 in LocalizationNetwork\n",
        "        self.localization_fc2.weight.data.fill_(0)\n",
        "        \"\"\" see RARE paper Fig. 6 (a) \"\"\"\n",
        "        ctrl_pts_x = np.linspace(-1.0, 1.0, int(F / 2))\n",
        "        ctrl_pts_y_top = np.linspace(0.0, -1.0, num=int(F / 2))\n",
        "        ctrl_pts_y_bottom = np.linspace(1.0, 0.0, num=int(F / 2))\n",
        "        ctrl_pts_top = np.stack([ctrl_pts_x, ctrl_pts_y_top], axis=1)\n",
        "        ctrl_pts_bottom = np.stack([ctrl_pts_x, ctrl_pts_y_bottom], axis=1)\n",
        "        initial_bias = np.concatenate([ctrl_pts_top, ctrl_pts_bottom], axis=0)\n",
        "        self.localization_fc2.bias.data = torch.from_numpy(initial_bias).float().view(-1)\n",
        "\n",
        "    def forward(self, batch_I):\n",
        "        \"\"\"\n",
        "        input:     batch_I : Batch Input Image [batch_size x I_channel_num x I_height x I_width]\n",
        "        output:    batch_C_prime : Predicted coordinates of fiducial points for input batch [batch_size x F x 2]\n",
        "        \"\"\"\n",
        "        batch_size = batch_I.size(0)\n",
        "        features = self.conv(batch_I).view(batch_size, -1)\n",
        "        batch_C_prime = self.localization_fc2(self.localization_fc1(features)).view(batch_size, self.F, 2)\n",
        "        return batch_C_prime\n",
        "\n",
        "\n",
        "class GridGenerator(nn.Module):\n",
        "    \"\"\" Grid Generator of RARE, which produces P_prime by multipling T with P \"\"\"\n",
        "\n",
        "    def __init__(self, F, I_r_size):\n",
        "        \"\"\" Generate P_hat and inv_delta_C for later \"\"\"\n",
        "        super(GridGenerator, self).__init__()\n",
        "        self.eps = 1e-6\n",
        "        self.I_r_height, self.I_r_width = I_r_size\n",
        "        self.F = F\n",
        "        self.C = self._build_C(self.F)  # F x 2\n",
        "        self.P = self._build_P(self.I_r_width, self.I_r_height)\n",
        "        ## for multi-gpu, you need register buffer\n",
        "        self.register_buffer(\"inv_delta_C\", torch.tensor(self._build_inv_delta_C(self.F, self.C)).float())  # F+3 x F+3\n",
        "        self.register_buffer(\"P_hat\", torch.tensor(self._build_P_hat(self.F, self.C, self.P)).float())  # n x F+3\n",
        "        ## for fine-tuning with different image width, you may use below instead of self.register_buffer\n",
        "        #self.inv_delta_C = torch.tensor(self._build_inv_delta_C(self.F, self.C)).float().cuda()  # F+3 x F+3\n",
        "        #self.P_hat = torch.tensor(self._build_P_hat(self.F, self.C, self.P)).float().cuda()  # n x F+3\n",
        "\n",
        "    def _build_C(self, F):\n",
        "        \"\"\" Return coordinates of fiducial points in I_r; C \"\"\"\n",
        "        ctrl_pts_x = np.linspace(-1.0, 1.0, int(F / 2))\n",
        "        ctrl_pts_y_top = -1 * np.ones(int(F / 2))\n",
        "        ctrl_pts_y_bottom = np.ones(int(F / 2))\n",
        "        ctrl_pts_top = np.stack([ctrl_pts_x, ctrl_pts_y_top], axis=1)\n",
        "        ctrl_pts_bottom = np.stack([ctrl_pts_x, ctrl_pts_y_bottom], axis=1)\n",
        "        C = np.concatenate([ctrl_pts_top, ctrl_pts_bottom], axis=0)\n",
        "        return C  # F x 2\n",
        "\n",
        "    def _build_inv_delta_C(self, F, C):\n",
        "        \"\"\" Return inv_delta_C which is needed to calculate T \"\"\"\n",
        "        hat_C = np.zeros((F, F), dtype=float)  # F x F\n",
        "        for i in range(0, F):\n",
        "            for j in range(i, F):\n",
        "                r = np.linalg.norm(C[i] - C[j])\n",
        "                hat_C[i, j] = r\n",
        "                hat_C[j, i] = r\n",
        "        np.fill_diagonal(hat_C, 1)\n",
        "        hat_C = (hat_C ** 2) * np.log(hat_C)\n",
        "        # print(C.shape, hat_C.shape)\n",
        "        delta_C = np.concatenate(  # F+3 x F+3\n",
        "            [\n",
        "                np.concatenate([np.ones((F, 1)), C, hat_C], axis=1),  # F x F+3\n",
        "                np.concatenate([np.zeros((2, 3)), np.transpose(C)], axis=1),  # 2 x F+3\n",
        "                np.concatenate([np.zeros((1, 3)), np.ones((1, F))], axis=1)  # 1 x F+3\n",
        "            ],\n",
        "            axis=0\n",
        "        )\n",
        "        inv_delta_C = np.linalg.inv(delta_C)\n",
        "        return inv_delta_C  # F+3 x F+3\n",
        "\n",
        "    def _build_P(self, I_r_width, I_r_height):\n",
        "        I_r_grid_x = (np.arange(-I_r_width, I_r_width, 2) + 1.0) / I_r_width  # self.I_r_width\n",
        "        I_r_grid_y = (np.arange(-I_r_height, I_r_height, 2) + 1.0) / I_r_height  # self.I_r_height\n",
        "        P = np.stack(  # self.I_r_width x self.I_r_height x 2\n",
        "            np.meshgrid(I_r_grid_x, I_r_grid_y),\n",
        "            axis=2\n",
        "        )\n",
        "        return P.reshape([-1, 2])  # n (= self.I_r_width x self.I_r_height) x 2\n",
        "\n",
        "    def _build_P_hat(self, F, C, P):\n",
        "        n = P.shape[0]  # n (= self.I_r_width x self.I_r_height)\n",
        "        P_tile = np.tile(np.expand_dims(P, axis=1), (1, F, 1))  # n x 2 -> n x 1 x 2 -> n x F x 2\n",
        "        C_tile = np.expand_dims(C, axis=0)  # 1 x F x 2\n",
        "        P_diff = P_tile - C_tile  # n x F x 2\n",
        "        rbf_norm = np.linalg.norm(P_diff, ord=2, axis=2, keepdims=False)  # n x F\n",
        "        rbf = np.multiply(np.square(rbf_norm), np.log(rbf_norm + self.eps))  # n x F\n",
        "        P_hat = np.concatenate([np.ones((n, 1)), P, rbf], axis=1)\n",
        "        return P_hat  # n x F+3\n",
        "\n",
        "    def build_P_prime(self, batch_C_prime):\n",
        "        \"\"\" Generate Grid from batch_C_prime [batch_size x F x 2] \"\"\"\n",
        "        batch_size = batch_C_prime.size(0)\n",
        "        batch_inv_delta_C = self.inv_delta_C.repeat(batch_size, 1, 1)\n",
        "        batch_P_hat = self.P_hat.repeat(batch_size, 1, 1)\n",
        "        batch_C_prime_with_zeros = torch.cat((batch_C_prime, torch.zeros(\n",
        "            batch_size, 3, 2).float().to(device)), dim=1)  # batch_size x F+3 x 2\n",
        "        batch_T = torch.bmm(batch_inv_delta_C, batch_C_prime_with_zeros)  # batch_size x F+3 x 2\n",
        "        batch_P_prime = torch.bmm(batch_P_hat, batch_T)  # batch_size x n x 2\n",
        "        return batch_P_prime  # batch_size x n x 2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Extraction"
      ],
      "metadata": {
        "id": "Et_5m9pgOYka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class VGG_FeatureExtractor(nn.Module):\n",
        "    \"\"\" FeatureExtractor of CRNN (https://arxiv.org/pdf/1507.05717.pdf) \"\"\"\n",
        "\n",
        "    def __init__(self, input_channel, output_channel=512):\n",
        "        super(VGG_FeatureExtractor, self).__init__()\n",
        "        self.output_channel = [int(output_channel / 8), int(output_channel / 4),\n",
        "                               int(output_channel / 2), output_channel]  # [64, 128, 256, 512]\n",
        "        self.ConvNet = nn.Sequential(\n",
        "            nn.Conv2d(input_channel, self.output_channel[0], 3, 1, 1), nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2),  # 64x16x50\n",
        "            nn.Conv2d(self.output_channel[0], self.output_channel[1], 3, 1, 1), nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2),  # 128x8x25\n",
        "            nn.Conv2d(self.output_channel[1], self.output_channel[2], 3, 1, 1), nn.ReLU(True),  # 256x8x25\n",
        "            nn.Conv2d(self.output_channel[2], self.output_channel[2], 3, 1, 1), nn.ReLU(True),\n",
        "            nn.MaxPool2d((2, 1), (2, 1)),  # 256x4x25\n",
        "            nn.Conv2d(self.output_channel[2], self.output_channel[3], 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(self.output_channel[3]), nn.ReLU(True),  # 512x4x25\n",
        "            nn.Conv2d(self.output_channel[3], self.output_channel[3], 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(self.output_channel[3]), nn.ReLU(True),\n",
        "            nn.MaxPool2d((2, 1), (2, 1)),  # 512x2x25\n",
        "            nn.Conv2d(self.output_channel[3], self.output_channel[3], 2, 1, 0), nn.ReLU(True))  # 512x1x24\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.ConvNet(input)\n",
        "\n",
        "\n",
        "class RCNN_FeatureExtractor(nn.Module):\n",
        "    \"\"\" FeatureExtractor of GRCNN (https://papers.nips.cc/paper/6637-gated-recurrent-convolution-neural-network-for-ocr.pdf) \"\"\"\n",
        "\n",
        "    def __init__(self, input_channel, output_channel=512):\n",
        "        super(RCNN_FeatureExtractor, self).__init__()\n",
        "        self.output_channel = [int(output_channel / 8), int(output_channel / 4),\n",
        "                               int(output_channel / 2), output_channel]  # [64, 128, 256, 512]\n",
        "        self.ConvNet = nn.Sequential(\n",
        "            nn.Conv2d(input_channel, self.output_channel[0], 3, 1, 1), nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2),  # 64 x 16 x 50\n",
        "            GRCL(self.output_channel[0], self.output_channel[0], num_iteration=5, kernel_size=3, pad=1),\n",
        "            nn.MaxPool2d(2, 2),  # 64 x 8 x 25\n",
        "            GRCL(self.output_channel[0], self.output_channel[1], num_iteration=5, kernel_size=3, pad=1),\n",
        "            nn.MaxPool2d(2, (2, 1), (0, 1)),  # 128 x 4 x 26\n",
        "            GRCL(self.output_channel[1], self.output_channel[2], num_iteration=5, kernel_size=3, pad=1),\n",
        "            nn.MaxPool2d(2, (2, 1), (0, 1)),  # 256 x 2 x 27\n",
        "            nn.Conv2d(self.output_channel[2], self.output_channel[3], 2, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(self.output_channel[3]), nn.ReLU(True))  # 512 x 1 x 26\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.ConvNet(input)\n",
        "\n",
        "\n",
        "class ResNet_FeatureExtractor(nn.Module):\n",
        "    \"\"\" FeatureExtractor of FAN (http://openaccess.thecvf.com/content_ICCV_2017/papers/Cheng_Focusing_Attention_Towards_ICCV_2017_paper.pdf) \"\"\"\n",
        "\n",
        "    def __init__(self, input_channel, output_channel=512):\n",
        "        super(ResNet_FeatureExtractor, self).__init__()\n",
        "        self.ConvNet = ResNet(input_channel, output_channel, BasicBlock, [1, 2, 5, 3])\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.ConvNet(input)\n",
        "\n",
        "\n",
        "# For Gated RCNN\n",
        "class GRCL(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channel, output_channel, num_iteration, kernel_size, pad):\n",
        "        super(GRCL, self).__init__()\n",
        "        self.wgf_u = nn.Conv2d(input_channel, output_channel, 1, 1, 0, bias=False)\n",
        "        self.wgr_x = nn.Conv2d(output_channel, output_channel, 1, 1, 0, bias=False)\n",
        "        self.wf_u = nn.Conv2d(input_channel, output_channel, kernel_size, 1, pad, bias=False)\n",
        "        self.wr_x = nn.Conv2d(output_channel, output_channel, kernel_size, 1, pad, bias=False)\n",
        "\n",
        "        self.BN_x_init = nn.BatchNorm2d(output_channel)\n",
        "\n",
        "        self.num_iteration = num_iteration\n",
        "        self.GRCL = [GRCL_unit(output_channel) for _ in range(num_iteration)]\n",
        "        self.GRCL = nn.Sequential(*self.GRCL)\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\" The input of GRCL is consistant over time t, which is denoted by u(0)\n",
        "        thus wgf_u / wf_u is also consistant over time t.\n",
        "        \"\"\"\n",
        "        wgf_u = self.wgf_u(input)\n",
        "        wf_u = self.wf_u(input)\n",
        "        x = F.relu(self.BN_x_init(wf_u))\n",
        "\n",
        "        for i in range(self.num_iteration):\n",
        "            x = self.GRCL[i](wgf_u, self.wgr_x(x), wf_u, self.wr_x(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class GRCL_unit(nn.Module):\n",
        "\n",
        "    def __init__(self, output_channel):\n",
        "        super(GRCL_unit, self).__init__()\n",
        "        self.BN_gfu = nn.BatchNorm2d(output_channel)\n",
        "        self.BN_grx = nn.BatchNorm2d(output_channel)\n",
        "        self.BN_fu = nn.BatchNorm2d(output_channel)\n",
        "        self.BN_rx = nn.BatchNorm2d(output_channel)\n",
        "        self.BN_Gx = nn.BatchNorm2d(output_channel)\n",
        "\n",
        "    def forward(self, wgf_u, wgr_x, wf_u, wr_x):\n",
        "        G_first_term = self.BN_gfu(wgf_u)\n",
        "        G_second_term = self.BN_grx(wgr_x)\n",
        "        G = F.sigmoid(G_first_term + G_second_term)\n",
        "\n",
        "        x_first_term = self.BN_fu(wf_u)\n",
        "        x_second_term = self.BN_Gx(self.BN_rx(wr_x) * G)\n",
        "        x = F.relu(x_first_term + x_second_term)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = self._conv3x3(inplanes, planes)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = self._conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def _conv3x3(self, in_planes, out_planes, stride=1):\n",
        "        \"3x3 convolution with padding\"\n",
        "        return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                         padding=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channel, output_channel, block, layers):\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        self.output_channel_block = [int(output_channel / 4), int(output_channel / 2), output_channel, output_channel]\n",
        "\n",
        "        self.inplanes = int(output_channel / 8)\n",
        "        self.conv0_1 = nn.Conv2d(input_channel, int(output_channel / 16),\n",
        "                                 kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn0_1 = nn.BatchNorm2d(int(output_channel / 16))\n",
        "        self.conv0_2 = nn.Conv2d(int(output_channel / 16), self.inplanes,\n",
        "                                 kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn0_2 = nn.BatchNorm2d(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.layer1 = self._make_layer(block, self.output_channel_block[0], layers[0])\n",
        "        self.conv1 = nn.Conv2d(self.output_channel_block[0], self.output_channel_block[\n",
        "                               0], kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(self.output_channel_block[0])\n",
        "\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.layer2 = self._make_layer(block, self.output_channel_block[1], layers[1], stride=1)\n",
        "        self.conv2 = nn.Conv2d(self.output_channel_block[1], self.output_channel_block[\n",
        "                               1], kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(self.output_channel_block[1])\n",
        "\n",
        "        self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=(2, 1), padding=(0, 1))\n",
        "        self.layer3 = self._make_layer(block, self.output_channel_block[2], layers[2], stride=1)\n",
        "        self.conv3 = nn.Conv2d(self.output_channel_block[2], self.output_channel_block[\n",
        "                               2], kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.output_channel_block[2])\n",
        "\n",
        "        self.layer4 = self._make_layer(block, self.output_channel_block[3], layers[3], stride=1)\n",
        "        self.conv4_1 = nn.Conv2d(self.output_channel_block[3], self.output_channel_block[\n",
        "                                 3], kernel_size=2, stride=(2, 1), padding=(0, 1), bias=False)\n",
        "        self.bn4_1 = nn.BatchNorm2d(self.output_channel_block[3])\n",
        "        self.conv4_2 = nn.Conv2d(self.output_channel_block[3], self.output_channel_block[\n",
        "                                 3], kernel_size=2, stride=1, padding=0, bias=False)\n",
        "        self.bn4_2 = nn.BatchNorm2d(self.output_channel_block[3])\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv0_1(x)\n",
        "        x = self.bn0_1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv0_2(x)\n",
        "        x = self.bn0_2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.maxpool1(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.maxpool2(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.maxpool3(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.layer4(x)\n",
        "        x = self.conv4_1(x)\n",
        "        x = self.bn4_1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv4_2(x)\n",
        "        x = self.bn4_2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "YLzi104SOYvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sequence Modeling "
      ],
      "metadata": {
        "id": "-XBh50YGOZFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class BidirectionalLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(BidirectionalLSTM, self).__init__()\n",
        "        self.rnn = nn.LSTM(input_size, hidden_size, bidirectional=True, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size * 2, output_size)\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        input : visual feature [batch_size x T x input_size]\n",
        "        output : contextual feature [batch_size x T x output_size]\n",
        "        \"\"\"\n",
        "        self.rnn.flatten_parameters()\n",
        "        recurrent, _ = self.rnn(input)  # batch_size x T x input_size -> batch_size x T x (2*hidden_size)\n",
        "        output = self.linear(recurrent)  # batch_size x T x output_size\n",
        "        return output"
      ],
      "metadata": {
        "id": "9DeIiJjaOZR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sequence prediction"
      ],
      "metadata": {
        "id": "GYgz6JSVQKZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(Attention, self).__init__()\n",
        "        self.attention_cell = AttentionCell(input_size, hidden_size, num_classes)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_classes = num_classes\n",
        "        self.generator = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def _char_to_onehot(self, input_char, onehot_dim=38):\n",
        "        input_char = input_char.unsqueeze(1)\n",
        "        batch_size = input_char.size(0)\n",
        "        one_hot = torch.FloatTensor(batch_size, onehot_dim).zero_().to(device)\n",
        "        one_hot = one_hot.scatter_(1, input_char, 1)\n",
        "        return one_hot\n",
        "\n",
        "    def forward(self, batch_H, text, is_train=True, batch_max_length=25):\n",
        "        \"\"\"\n",
        "        input:\n",
        "            batch_H : contextual_feature H = hidden state of encoder. [batch_size x num_steps x contextual_feature_channels]\n",
        "            text : the text-index of each image. [batch_size x (max_length+1)]. +1 for [GO] token. text[:, 0] = [GO].\n",
        "        output: probability distribution at each step [batch_size x num_steps x num_classes]\n",
        "        \"\"\"\n",
        "        batch_size = batch_H.size(0)\n",
        "        num_steps = batch_max_length + 1  # +1 for [s] at end of sentence.\n",
        "\n",
        "        output_hiddens = torch.FloatTensor(batch_size, num_steps, self.hidden_size).fill_(0).to(device)\n",
        "        hidden = (torch.FloatTensor(batch_size, self.hidden_size).fill_(0).to(device),\n",
        "                  torch.FloatTensor(batch_size, self.hidden_size).fill_(0).to(device))\n",
        "\n",
        "        if is_train:\n",
        "            for i in range(num_steps):\n",
        "                # one-hot vectors for a i-th char. in a batch\n",
        "                char_onehots = self._char_to_onehot(text[:, i], onehot_dim=self.num_classes)\n",
        "                # hidden : decoder's hidden s_{t-1}, batch_H : encoder's hidden H, char_onehots : one-hot(y_{t-1})\n",
        "                hidden, alpha = self.attention_cell(hidden, batch_H, char_onehots)\n",
        "                output_hiddens[:, i, :] = hidden[0]  # LSTM hidden index (0: hidden, 1: Cell)\n",
        "            probs = self.generator(output_hiddens)\n",
        "\n",
        "        else:\n",
        "            targets = torch.LongTensor(batch_size).fill_(0).to(device)  # [GO] token\n",
        "            probs = torch.FloatTensor(batch_size, num_steps, self.num_classes).fill_(0).to(device)\n",
        "\n",
        "            for i in range(num_steps):\n",
        "                char_onehots = self._char_to_onehot(targets, onehot_dim=self.num_classes)\n",
        "                hidden, alpha = self.attention_cell(hidden, batch_H, char_onehots)\n",
        "                probs_step = self.generator(hidden[0])\n",
        "                probs[:, i, :] = probs_step\n",
        "                _, next_input = probs_step.max(1)\n",
        "                targets = next_input\n",
        "\n",
        "        return probs  # batch_size x num_steps x num_classes\n",
        "\n",
        "\n",
        "class AttentionCell(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, num_embeddings):\n",
        "        super(AttentionCell, self).__init__()\n",
        "        self.i2h = nn.Linear(input_size, hidden_size, bias=False)\n",
        "        self.h2h = nn.Linear(hidden_size, hidden_size)  # either i2i or h2h should have bias\n",
        "        self.score = nn.Linear(hidden_size, 1, bias=False)\n",
        "        self.rnn = nn.LSTMCell(input_size + num_embeddings, hidden_size)\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    def forward(self, prev_hidden, batch_H, char_onehots):\n",
        "        # [batch_size x num_encoder_step x num_channel] -> [batch_size x num_encoder_step x hidden_size]\n",
        "        batch_H_proj = self.i2h(batch_H)\n",
        "        prev_hidden_proj = self.h2h(prev_hidden[0]).unsqueeze(1)\n",
        "        e = self.score(torch.tanh(batch_H_proj + prev_hidden_proj))  # batch_size x num_encoder_step * 1\n",
        "\n",
        "        alpha = F.softmax(e, dim=1)\n",
        "        context = torch.bmm(alpha.permute(0, 2, 1), batch_H).squeeze(1)  # batch_size x num_channel\n",
        "        concat_context = torch.cat([context, char_onehots], 1)  # batch_size x (num_channel + num_embedding)\n",
        "        cur_hidden = self.rnn(concat_context, prev_hidden)\n",
        "        return cur_hidden, alpha"
      ],
      "metadata": {
        "id": "8-6ne74RQKk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Classification modeling"
      ],
      "metadata": {
        "id": "TVXiZZRkOZfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassificationNet(nn.Module):\n",
        "    def __init__(self, feature_input_size, head1_nodes, head2_nodes, head3_nodes, label1, label2, label3):                #3 more labels\n",
        "      super(ClassificationNet, self).__init__()\n",
        "                                                                                                      #modify it acc to attention cell\n",
        "      #self.resnet_model = model_core                                                              #no need  \n",
        "\n",
        "                                                                                \n",
        "      self.x1 = nn.Linear(feature_input_size, 256)\n",
        "      nn.init.xavier_normal_(self.x1.weight)\n",
        "      self.x2 = nn.Linear(256, 256)\n",
        "      nn.init.xavier_normal_(self.x1.weight)\n",
        "\n",
        "\n",
        "\n",
        "      #HEADS\n",
        "      self.fc1 = nn.Linear(256, 128)\n",
        "      nn.init.xavier_normal_(self.fc1.weight)\n",
        "      self.head1 = nn.Linear(128, head1_nodes)                #head1_nodes = 1 ; sigmoid activation; binary cross entropy (HSRP or non HSRP)\n",
        "      nn.init.xavier_normal_(self.head1.weight)\n",
        "\n",
        "\n",
        "      self.fc2 = nn.Linear(256, 128)\n",
        "      nn.init.xavier_normal_(self.fc2.weight)\n",
        "      self.head2 = nn.Linear(128, head2_nodes)                #head2_nodes = 4 ; softmax ; categorical cross entropy  (languages : Eng, Hindi, Marathi, Nepali)\n",
        "      nn.init.xavier_normal_(self.head2.weight)\n",
        "\n",
        "      self.fc3 = nn.Linear(256, 128)\n",
        "      nn.init.xavier_normal_(self.fc3.weight)\n",
        "      self.head3 = nn.Linear(128, head3_nodes)                #head3_nodes = 7 ; softmax ; categorical cross entropy (types of number plates: 7)\n",
        "      nn.init.xavier_normal_(self.head3.weight)\n",
        "\n",
        "\n",
        "#https://wandb.ai/ayush-thakur/dl-question-bank/reports/An-Introduction-To-The-PyTorch-View-Function--VmlldzoyMDM0Nzg    why x.view(-1, dimensions of the features from resent)\n",
        "#https://www.analyticsvidhya.com/blog/2019/10/building-image-classification-models-cnn-pytorch/\n",
        "\n",
        "    # x represents our data\n",
        "    def forward(self, x):                                             # add 3 more labels\n",
        "       \n",
        "      #output = output.view(input_imgs.size(0), -1)                                                          ##  x1 = self.resnet_model(x)     no need\n",
        "      x1 = x.view(x.size(0), -1)                              #flattening the input as resnet output channel =512         \n",
        "      x1 = F.relu(self.x1(x1))\n",
        "      x1 = F.relu(self.x2(x1))\n",
        "    \n",
        "                                                                    #write forward function just the way it's in Attention module\n",
        "      # Apply softmax to x\n",
        "      h1 = F.relu(self.fc1(x1))\n",
        "      head1 = F.softmax(self.head1(h1), dim=1)\n",
        "\n",
        "      h2 = F.relu(self.fc2(x1))\n",
        "      head2 = F.softmax(self.head2(h2), dim=1)\n",
        "\n",
        "      h3 = F.relu(self.fc3(x1))\n",
        "      head3 = F.softmax(self.head3(h3), dim=1)\n",
        "\n",
        "      return head1, head2, head3\n",
        "\n",
        "#cls_module = ClassificationNet(\"\"\"add resnet50 model here\"\"\")"
      ],
      "metadata": {
        "id": "5WbBmrZ4OZsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating dataset"
      ],
      "metadata": {
        "id": "V0UmWk-5P6Pe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" a modified version of CRNN torch repository https://github.com/bgshih/crnn/blob/master/tool/create_dataset.py \"\"\"\n",
        "\n",
        "import fire\n",
        "import os\n",
        "import lmdb\n",
        "import cv2\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def checkImageIsValid(imageBin):\n",
        "    if imageBin is None:\n",
        "        return False\n",
        "    imageBuf = np.frombuffer(imageBin, dtype=np.uint8)\n",
        "    img = cv2.imdecode(imageBuf, cv2.IMREAD_GRAYSCALE)\n",
        "    imgH, imgW = img.shape[0], img.shape[1]\n",
        "    if imgH * imgW == 0:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def writeCache(env, cache):\n",
        "    with env.begin(write=True) as txn:\n",
        "        for k, v in cache.items():\n",
        "            txn.put(k, v)\n",
        "\n",
        "\n",
        "def createDataset(inputPath, gtFile, outputPath, checkValid=True):\n",
        "    \"\"\"\n",
        "    Create LMDB dataset for training and evaluation.\n",
        "    ARGS:\n",
        "        inputPath  : input folder path where starts imagePath\n",
        "        outputPath : LMDB output path\n",
        "        gtFile     : list of image path and label\n",
        "        checkValid : if true, check the validity of every image\n",
        "    \"\"\"\n",
        "    os.makedirs(outputPath, exist_ok=True)\n",
        "    env = lmdb.open(outputPath, map_size=1099511627776)\n",
        "    cache = {}\n",
        "    cnt = 1\n",
        "\n",
        "    with open(gtFile, 'r', encoding='utf-8') as data:\n",
        "        datalist = data.readlines()\n",
        "\n",
        "    nSamples = len(datalist)\n",
        "    for i in range(nSamples):\n",
        "        imagePath, label = datalist[i].strip('\\n').split('\\t')\n",
        "        imagePath = os.path.join(inputPath, imagePath)\n",
        "\n",
        "        # # only use alphanumeric data\n",
        "        # if re.search('[^a-zA-Z0-9]', label):\n",
        "        #     continue\n",
        "\n",
        "        if not os.path.exists(imagePath):\n",
        "            print('%s does not exist' % imagePath)\n",
        "            continue\n",
        "        with open(imagePath, 'rb') as f:\n",
        "            imageBin = f.read()\n",
        "        if checkValid:\n",
        "            try:\n",
        "                if not checkImageIsValid(imageBin):\n",
        "                    print('%s is not a valid image' % imagePath)\n",
        "                    continue\n",
        "            except:\n",
        "                print('error occured', i)\n",
        "                with open(outputPath + '/error_image_log.txt', 'a') as log:\n",
        "                    log.write('%s-th image data occured error\\n' % str(i))\n",
        "                continue\n",
        "\n",
        "        imageKey = 'image-%09d'.encode() % cnt\n",
        "        labelKey = 'label-%09d'.encode() % cnt\n",
        "        cache[imageKey] = imageBin\n",
        "        cache[labelKey] = label.encode() # if we consider label format --> text_label1_label2_label3     then no need to modify this\n",
        "                                         # but if we seperate the label into 4 labels then.... need to modify\n",
        "\n",
        "        if cnt % 1000 == 0:\n",
        "            writeCache(env, cache)\n",
        "            cache = {}\n",
        "            print('Written %d / %d' % (cnt, nSamples))\n",
        "        cnt += 1\n",
        "    nSamples = cnt-1\n",
        "    cache['num-samples'.encode()] = str(nSamples).encode()\n",
        "    writeCache(env, cache)\n",
        "    print('Created dataset with %d samples' % nSamples)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    fire.Fire(createDataset)"
      ],
      "metadata": {
        "id": "-ApU6YpVP6ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset "
      ],
      "metadata": {
        "id": "K7108z12Qlxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import re\n",
        "import six\n",
        "import math\n",
        "import lmdb\n",
        "import torch\n",
        "\n",
        "from natsort import natsorted\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, ConcatDataset, Subset\n",
        "from torch._utils import _accumulate\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "class Batch_Balanced_Dataset(object):\n",
        "\n",
        "    def __init__(self, opt):\n",
        "        \"\"\"\n",
        "        Modulate the data ratio in the batch.\n",
        "        For example, when select_data is \"MJ-ST\" and batch_ratio is \"0.5-0.5\",\n",
        "        the 50% of the batch is filled with MJ and the other 50% of the batch is filled with ST.\n",
        "        \"\"\"\n",
        "        log = open(f'./saved_models/{opt.exp_name}/log_dataset.txt', 'a')\n",
        "        dashed_line = '-' * 80\n",
        "        print(dashed_line)\n",
        "        log.write(dashed_line + '\\n')\n",
        "        print(f'dataset_root: {opt.train_data}\\nopt.select_data: {opt.select_data}\\nopt.batch_ratio: {opt.batch_ratio}')\n",
        "        log.write(f'dataset_root: {opt.train_data}\\nopt.select_data: {opt.select_data}\\nopt.batch_ratio: {opt.batch_ratio}\\n')\n",
        "        assert len(opt.select_data) == len(opt.batch_ratio)\n",
        "\n",
        "        _AlignCollate = AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=opt.PAD)\n",
        "        self.data_loader_list = []\n",
        "        self.dataloader_iter_list = []\n",
        "        batch_size_list = []\n",
        "        Total_batch_size = 0\n",
        "        for selected_d, batch_ratio_d in zip(opt.select_data, opt.batch_ratio):\n",
        "            _batch_size = max(round(opt.batch_size * float(batch_ratio_d)), 1)\n",
        "            print(dashed_line)\n",
        "            log.write(dashed_line + '\\n')\n",
        "            _dataset, _dataset_log = hierarchical_dataset(root=opt.train_data, opt=opt, select_data=[selected_d])\n",
        "            total_number_dataset = len(_dataset)\n",
        "            log.write(_dataset_log)\n",
        "\n",
        "            \"\"\"\n",
        "            The total number of data can be modified with opt.total_data_usage_ratio.\n",
        "            ex) opt.total_data_usage_ratio = 1 indicates 100% usage, and 0.2 indicates 20% usage.\n",
        "            See 4.2 section in our paper.\n",
        "            \"\"\"\n",
        "            number_dataset = int(total_number_dataset * float(opt.total_data_usage_ratio))\n",
        "            dataset_split = [number_dataset, total_number_dataset - number_dataset]\n",
        "            indices = range(total_number_dataset)\n",
        "            _dataset, _ = [Subset(_dataset, indices[offset - length:offset])\n",
        "                           for offset, length in zip(_accumulate(dataset_split), dataset_split)]\n",
        "            selected_d_log = f'num total samples of {selected_d}: {total_number_dataset} x {opt.total_data_usage_ratio} (total_data_usage_ratio) = {len(_dataset)}\\n'\n",
        "            selected_d_log += f'num samples of {selected_d} per batch: {opt.batch_size} x {float(batch_ratio_d)} (batch_ratio) = {_batch_size}'\n",
        "            print(selected_d_log)\n",
        "            log.write(selected_d_log + '\\n')\n",
        "            batch_size_list.append(str(_batch_size))\n",
        "            Total_batch_size += _batch_size\n",
        "\n",
        "            _data_loader = torch.utils.data.DataLoader(\n",
        "                _dataset, batch_size=_batch_size,\n",
        "                shuffle=True,\n",
        "                num_workers=int(opt.workers),\n",
        "                collate_fn=_AlignCollate, pin_memory=True)\n",
        "            self.data_loader_list.append(_data_loader)\n",
        "            self.dataloader_iter_list.append(iter(_data_loader))\n",
        "\n",
        "        Total_batch_size_log = f'{dashed_line}\\n'\n",
        "        batch_size_sum = '+'.join(batch_size_list)\n",
        "        Total_batch_size_log += f'Total_batch_size: {batch_size_sum} = {Total_batch_size}\\n'\n",
        "        Total_batch_size_log += f'{dashed_line}'\n",
        "        opt.batch_size = Total_batch_size\n",
        "\n",
        "        print(Total_batch_size_log)\n",
        "        log.write(Total_batch_size_log + '\\n')\n",
        "        log.close()\n",
        "\n",
        "    def get_batch(self):\n",
        "        balanced_batch_images = []\n",
        "        balanced_batch_texts = []\n",
        "\n",
        "        for i, data_loader_iter in enumerate(self.dataloader_iter_list):\n",
        "            try:\n",
        "                image, text = data_loader_iter.next()\n",
        "                balanced_batch_images.append(image)\n",
        "                balanced_batch_texts += text\n",
        "            except StopIteration:\n",
        "                self.dataloader_iter_list[i] = iter(self.data_loader_list[i])\n",
        "                image, text = self.dataloader_iter_list[i].next()\n",
        "                balanced_batch_images.append(image)\n",
        "                balanced_batch_texts += text           #text.split(_)[0]    # JUst need to \n",
        "            except ValueError:\n",
        "                pass\n",
        "\n",
        "        balanced_batch_images = torch.cat(balanced_batch_images, 0)\n",
        "\n",
        "        return balanced_batch_images, balanced_batch_texts    #GJ8990_English_HSRP_Private-non-electric\n",
        "\n",
        "#FOR IMAGE\n",
        "def hierarchical_dataset(root, opt, select_data='/'):\n",
        "    \"\"\" select_data='/' contains all sub-directory of root directory \"\"\"\n",
        "    dataset_list = []\n",
        "    dataset_log = f'dataset_root:    {root}\\t dataset: {select_data[0]}'\n",
        "    print(dataset_log)\n",
        "    dataset_log += '\\n'\n",
        "    for dirpath, dirnames, filenames in os.walk(root+'/'):\n",
        "        if not dirnames:\n",
        "            select_flag = False\n",
        "            for selected_d in select_data:\n",
        "                if selected_d in dirpath:\n",
        "                    select_flag = True\n",
        "                    break\n",
        "\n",
        "            if select_flag:\n",
        "                dataset = LmdbDataset(dirpath, opt)\n",
        "                sub_dataset_log = f'sub-directory:\\t/{os.path.relpath(dirpath, root)}\\t num samples: {len(dataset)}'\n",
        "                print(sub_dataset_log)\n",
        "                dataset_log += f'{sub_dataset_log}\\n'\n",
        "                dataset_list.append(dataset)\n",
        "\n",
        "    concatenated_dataset = ConcatDataset(dataset_list)\n",
        "\n",
        "    return concatenated_dataset, dataset_log\n",
        "\n",
        "\n",
        "class LmdbDataset(Dataset):\n",
        "\n",
        "    def __init__(self, root, opt):\n",
        "\n",
        "        self.root = root\n",
        "        self.opt = opt\n",
        "        self.env = lmdb.open(root, max_readers=32, readonly=True, lock=False, readahead=False, meminit=False)\n",
        "        if not self.env:\n",
        "            print('cannot create lmdb from %s' % (root))\n",
        "            sys.exit(0)\n",
        "\n",
        "        with self.env.begin(write=False) as txn:\n",
        "            nSamples = int(txn.get('num-samples'.encode()))\n",
        "            self.nSamples = nSamples\n",
        "\n",
        "            if self.opt.data_filtering_off:\n",
        "                # for fast check or benchmark evaluation with no filtering\n",
        "                self.filtered_index_list = [index + 1 for index in range(self.nSamples)]\n",
        "            else:\n",
        "                \"\"\" Filtering part\n",
        "                If you want to evaluate IC15-2077 & CUTE datasets which have special character labels,\n",
        "                use --data_filtering_off and only evaluate on alphabets and digits.\n",
        "                see https://github.com/clovaai/deep-text-recognition-benchmark/blob/6593928855fb7abb999a99f428b3e4477d4ae356/dataset.py#L190-L192\n",
        "                And if you want to evaluate them with the model trained with --sensitive option,\n",
        "                use --sensitive and --data_filtering_off,\n",
        "                see https://github.com/clovaai/deep-text-recognition-benchmark/blob/dff844874dbe9e0ec8c5a52a7bd08c7f20afe704/test.py#L137-L144\n",
        "                \"\"\"\n",
        "                self.filtered_index_list = []\n",
        "                for index in range(self.nSamples):\n",
        "                    index += 1  # lmdb starts with 1\n",
        "                    label_key = 'label-%09d'.encode() % index\n",
        "                    label = txn.get(label_key).decode('utf-8')\n",
        "\n",
        "                    if len(label) > self.opt.batch_max_length:\n",
        "                        # print(f'The length of the label is longer than max_length: length\n",
        "                        # {len(label)}, {label} in dataset {self.root}')\n",
        "                        continue\n",
        "\n",
        "                    # By default, images containing characters which are not in opt.character are filtered.\n",
        "                    # You can add [UNK] token to `opt.character` in utils.py instead of this filtering.\n",
        "                    out_of_char = f'[^{self.opt.character}]'\n",
        "                    if re.search(out_of_char, label.lower()):\n",
        "                        continue\n",
        "\n",
        "                    self.filtered_index_list.append(index)\n",
        "\n",
        "                self.nSamples = len(self.filtered_index_list)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.nSamples\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        assert index <= len(self), 'index range error'\n",
        "        index = self.filtered_index_list[index]\n",
        "\n",
        "        with self.env.begin(write=False) as txn:\n",
        "            label_key = 'label-%09d'.encode() % index\n",
        "            label = txn.get(label_key).decode('utf-8')\n",
        "            img_key = 'image-%09d'.encode() % index\n",
        "            imgbuf = txn.get(img_key)\n",
        "\n",
        "            buf = six.BytesIO()\n",
        "            buf.write(imgbuf)\n",
        "            buf.seek(0)\n",
        "            try:\n",
        "                if self.opt.rgb:\n",
        "                    img = Image.open(buf).convert('RGB')  # for color image\n",
        "                else:\n",
        "                    img = Image.open(buf).convert('L')\n",
        "\n",
        "            except IOError:\n",
        "                print(f'Corrupted image for {index}')\n",
        "                # make dummy image and dummy label for corrupted image.\n",
        "                if self.opt.rgb:\n",
        "                    img = Image.new('RGB', (self.opt.imgW, self.opt.imgH))\n",
        "                else:\n",
        "                    img = Image.new('L', (self.opt.imgW, self.opt.imgH))\n",
        "                label = '[dummy_label]'\n",
        "\n",
        "            if not self.opt.sensitive:\n",
        "                label = label.lower()\n",
        "\n",
        "            # We only train and evaluate on alphanumerics (or pre-defined character set in train.py)\n",
        "            out_of_char = f'[^{self.opt.character}]'\n",
        "            label = re.sub(out_of_char, '', label)\n",
        "\n",
        "        return (img, label)               #dataloader--> hsrp: 0/1; lang: binary list [0,0,1,0]; similar to types\n",
        "\n",
        "\n",
        "\n",
        "#FOR IMAGE\n",
        "class RawDataset(Dataset):\n",
        "\n",
        "    def __init__(self, root, opt):\n",
        "        self.opt = opt\n",
        "        self.image_path_list = []\n",
        "        for dirpath, dirnames, filenames in os.walk(root):\n",
        "            for name in filenames:\n",
        "                _, ext = os.path.splitext(name)\n",
        "                ext = ext.lower()\n",
        "                if ext == '.jpg' or ext == '.jpeg' or ext == '.png':\n",
        "                    self.image_path_list.append(os.path.join(dirpath, name))\n",
        "\n",
        "        self.image_path_list = natsorted(self.image_path_list)\n",
        "        self.nSamples = len(self.image_path_list)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.nSamples\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        try:\n",
        "            if self.opt.rgb:\n",
        "                img = Image.open(self.image_path_list[index]).convert('RGB')  # for color image\n",
        "            else:\n",
        "                img = Image.open(self.image_path_list[index]).convert('L')\n",
        "\n",
        "        except IOError:\n",
        "            print(f'Corrupted image for {index}')\n",
        "            # make dummy image and dummy label for corrupted image.\n",
        "            if self.opt.rgb:\n",
        "                img = Image.new('RGB', (self.opt.imgW, self.opt.imgH))\n",
        "            else:\n",
        "                img = Image.new('L', (self.opt.imgW, self.opt.imgH))\n",
        "\n",
        "        return (img, self.image_path_list[index])\n",
        "\n",
        "\n",
        "#FOR IMAGE\n",
        "class ResizeNormalize(object):\n",
        "\n",
        "    def __init__(self, size, interpolation=Image.BICUBIC):\n",
        "        self.size = size\n",
        "        self.interpolation = interpolation\n",
        "        self.toTensor = transforms.ToTensor()\n",
        "\n",
        "    def __call__(self, img):\n",
        "        img = img.resize(self.size, self.interpolation)\n",
        "        img = self.toTensor(img)\n",
        "        img.sub_(0.5).div_(0.5)\n",
        "        return img\n",
        "\n",
        "\n",
        "#FOR IMAGE\n",
        "class NormalizePAD(object):\n",
        "\n",
        "    def __init__(self, max_size, PAD_type='right'):\n",
        "        self.toTensor = transforms.ToTensor()\n",
        "        self.max_size = max_size\n",
        "        self.max_width_half = math.floor(max_size[2] / 2)\n",
        "        self.PAD_type = PAD_type\n",
        "\n",
        "    def __call__(self, img):\n",
        "        img = self.toTensor(img)\n",
        "        img.sub_(0.5).div_(0.5)\n",
        "        c, h, w = img.size()\n",
        "        Pad_img = torch.FloatTensor(*self.max_size).fill_(0)\n",
        "        Pad_img[:, :, :w] = img  # right pad\n",
        "        if self.max_size[2] != w:  # add border Pad\n",
        "            Pad_img[:, :, w:] = img[:, :, w - 1].unsqueeze(2).expand(c, h, self.max_size[2] - w)\n",
        "\n",
        "        return Pad_img\n",
        "\n",
        "\n",
        "class AlignCollate(object):\n",
        "\n",
        "    def __init__(self, imgH=32, imgW=100, keep_ratio_with_pad=False):\n",
        "        self.imgH = imgH\n",
        "        self.imgW = imgW\n",
        "        self.keep_ratio_with_pad = keep_ratio_with_pad\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        batch = filter(lambda x: x is not None, batch)\n",
        "        images, labels = zip(*batch)\n",
        "\n",
        "        if self.keep_ratio_with_pad:  # same concept with 'Rosetta' paper\n",
        "            resized_max_w = self.imgW\n",
        "            input_channel = 3 if images[0].mode == 'RGB' else 1\n",
        "            transform = NormalizePAD((input_channel, self.imgH, resized_max_w))\n",
        "\n",
        "            resized_images = []\n",
        "            for image in images:\n",
        "                w, h = image.size\n",
        "                ratio = w / float(h)\n",
        "                if math.ceil(self.imgH * ratio) > self.imgW:\n",
        "                    resized_w = self.imgW\n",
        "                else:\n",
        "                    resized_w = math.ceil(self.imgH * ratio)\n",
        "\n",
        "                resized_image = image.resize((resized_w, self.imgH), Image.BICUBIC)\n",
        "                resized_images.append(transform(resized_image))\n",
        "                # resized_image.save('./image_test/%d_test.jpg' % w)\n",
        "\n",
        "            image_tensors = torch.cat([t.unsqueeze(0) for t in resized_images], 0)\n",
        "\n",
        "        else:\n",
        "            transform = ResizeNormalize((self.imgW, self.imgH))\n",
        "            image_tensors = [transform(image) for image in images]\n",
        "            image_tensors = torch.cat([t.unsqueeze(0) for t in image_tensors], 0)\n",
        "\n",
        "        return image_tensors, labels\n",
        "\n",
        "\n",
        "\n",
        "#FOR IMAGE\n",
        "def tensor2im(image_tensor, imtype=np.uint8):\n",
        "    image_numpy = image_tensor.cpu().float().numpy()\n",
        "    if image_numpy.shape[0] == 1:\n",
        "        image_numpy = np.tile(image_numpy, (3, 1, 1))\n",
        "    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n",
        "    return image_numpy.astype(imtype)\n",
        "\n",
        "\n",
        "\n",
        "#FOR IMAGE\n",
        "def save_image(image_numpy, image_path):                                 \n",
        "    image_pil = Image.fromarray(image_numpy)\n",
        "    image_pil.save(image_path)"
      ],
      "metadata": {
        "id": "ZfJHhKHAQmI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating dataset for cls module"
      ],
      "metadata": {
        "id": "TONLfaK81DDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class AnimeMTLDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_folder, csv_file_path, transformations, is_training_set = True) :\n",
        "        super().__init__()\n",
        "        \n",
        "        self.path = csv_file_path \n",
        "        self.transforms = transformations\n",
        "        self.is_training_set = is_training_set\n",
        "        self.image_folder = image_folder\n",
        "        self.dataset = {}\n",
        "        self.column_names = None\n",
        "\n",
        "        if self.is_training_set:\n",
        "            # read the csv file into a dictionary\n",
        "            with open(csv_file_path, 'r') as csv_file :\n",
        "                # now we have a generator that when called\n",
        "                # will read one line. \n",
        "                csv_reader = csv.reader(csv_file)\n",
        "                # to skip header we simply do next(csv_reader)\n",
        "                # but since column names can be useful for us\n",
        "                # later on, we take advatnage of this and also\n",
        "                # save the header!\n",
        "                self.column_names = next(csv_reader)\n",
        "                # read each record into our dictionary\n",
        "                # each record(line) is a list containing all columns\n",
        "                for i, line in enumerate(csv_reader):\n",
        "                    self.dataset[i] = line\n",
        "        else:\n",
        "            self.image_folder = os.path.join(self.image_folder, 'test')\n",
        "            for i, img_path in enumerate(os.listdir(self.image_folder)):\n",
        "                self.dataset[i] = img_path\n",
        "\n",
        "    def _format_input(self, input_str, one_hot=False):\n",
        "        one_hot_tensor = torch.tensor([float(i) for i in input_str])\n",
        "        if one_hot: \n",
        "            return one_hot_tensor \n",
        "        if one_hot_tensor.size(0) > 1 : \n",
        "            return torch.argmax(one_hot_tensor)\n",
        "        else:\n",
        "            return one_hot_tensor[0].int()\n",
        "        \n",
        "    # lets create the corsponding labels for each category\n",
        "    def _parse_labels(self, input_str):\n",
        "        # white,red,green,black,blue,purple,gold,silver\n",
        "        colors = self._format_input(input_str[4:12], True)            \n",
        "        # gender_Female,gender_Male\n",
        "        genders = self._format_input(input_str[12:14])        \n",
        "        # region_Asia, region_Egypt, region_Europe, region_Middle East  \n",
        "        regions = self._format_input(input_str[14:18])        \n",
        "        # fighting_type_magic, fighting_type_melee, fighting_type_ranged\n",
        "        fighting_styles = self._format_input(input_str[18:21])          \n",
        "        # alignment_CE, alignment_CG, alignment_CN, alignment_LE,\n",
        "        # alignment_LG, alignment_LN, alignment_NE, alignment_NG, alignment_TN\n",
        "        alignments = self._format_input(input_str[21:])  \n",
        "        return colors, genders, regions, fighting_styles, alignments\n",
        "\n",
        "\n",
        "    # in getitem, we retrieve one item based on the input index\n",
        "    # thats why we used a ditionary to make it easier to fectch\n",
        "    # images\n",
        "    def __getitem__(self, index):\n",
        "        if self.is_training_set:\n",
        "            # we can access each category using a its corrosponding index\n",
        "            # each record is simply a list and therefore accessing is trivial\n",
        "            img_path = self.dataset[index][1]\n",
        "            # to get labels in proper form, we use a helper method here\n",
        "            labels = self._parse_labels(self.dataset[index])\n",
        "        else:\n",
        "            img_path = self.dataset[index]\n",
        "            labels = -1\n",
        "        # image files must be read as bytes so we use 'rb' instead of simply 'r' \n",
        "        # which is used for text files\n",
        "        with open(os.path.join(self.image_folder, img_path), 'rb') as img_file:\n",
        "            # since our datasets include png images, we need to make sure\n",
        "            # we read only 3 channels and not more!\n",
        "            img = Image.open(img_file).convert('RGB')\n",
        "            # apply the transformations \n",
        "            img = self.transforms(img)\n",
        "            return img, labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def Label_names(self):\n",
        "        #remove the _in names (i.e gender_male becomes male)\n",
        "        self.column_names = [name.split('_')[-1] if '_' in name else name\\\n",
        "                            for name in self.column_names ]\n",
        "        # white,red,green,black,blue,purple,gold,silver\n",
        "        color_names = self.column_names[4:12]\n",
        "        # gender_Female,gender_Male\n",
        "        gender_names = self.column_names[12:14]\n",
        "        # region_Asia, region_Egypt, region_Europe, region_Middle East  \n",
        "        region_names = self.column_names[14:18]        \n",
        "        # fighting_type_magic, fighting_type_melee, fighting_type_ranged\n",
        "        fighting_names = self.column_names[18:21]          \n",
        "        # alignment_CE, alignment_CG, alignment_CN, alignment_LE,\n",
        "        # alignment_LG, alignment_LN, alignment_NE, alignment_NG, alignment_TN\n",
        "        alignment_names = self.column_names[21:]  \n",
        "        return (color_names, gender_names, region_names, fighting_names, alignment_names)"
      ],
      "metadata": {
        "id": "DTZRe3Oh1DmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model"
      ],
      "metadata": {
        "id": "vVWGLgKdQl8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "from modules.transformation import TPS_SpatialTransformerNetwork\n",
        "from modules.feature_extraction import VGG_FeatureExtractor, RCNN_FeatureExtractor, ResNet_FeatureExtractor\n",
        "from modules.sequence_modeling import BidirectionalLSTM\n",
        "from modules.prediction import Attention\n",
        "from modules.classification_modeling import  ClassificationNet             #added      classification_modeling.py file\n",
        "\n",
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self, opt):\n",
        "        super(Model, self).__init__()\n",
        "        self.opt = opt\n",
        "        self.stages = {'Trans': opt.Transformation, 'Feat': opt.FeatureExtraction,\n",
        "                       'Seq': opt.SequenceModeling, 'Pred': opt.Prediction , 'Cls': opt.ClassificationModeling }  #added'Cls': opt.ClassificationModeling\n",
        "\n",
        "        \"\"\" Transformation \"\"\"\n",
        "        if opt.Transformation == 'TPS':\n",
        "            self.Transformation = TPS_SpatialTransformerNetwork(\n",
        "                F=opt.num_fiducial, I_size=(opt.imgH, opt.imgW), I_r_size=(opt.imgH, opt.imgW), I_channel_num=opt.input_channel)\n",
        "        else:\n",
        "            print('No Transformation module specified')\n",
        "\n",
        "        \"\"\" FeatureExtraction \"\"\"\n",
        "        if opt.FeatureExtraction == 'VGG':\n",
        "            self.FeatureExtraction = VGG_FeatureExtractor(opt.input_channel, opt.output_channel)\n",
        "        elif opt.FeatureExtraction == 'RCNN':\n",
        "            self.FeatureExtraction = RCNN_FeatureExtractor(opt.input_channel, opt.output_channel)\n",
        "        elif opt.FeatureExtraction == 'ResNet':\n",
        "            self.FeatureExtraction = ResNet_FeatureExtractor(opt.input_channel, opt.output_channel)\n",
        "        else:\n",
        "            raise Exception('No FeatureExtraction module specified')\n",
        "        self.FeatureExtraction_output = opt.output_channel  # int(imgH/16-1) * 512\n",
        "        self.AdaptiveAvgPool = nn.AdaptiveAvgPool2d((None, 1))  # Transform final (imgH/16-1) -> 1\n",
        "                                                                                                                              \n",
        "        \"\"\" Sequence modeling\"\"\"\n",
        "        if opt.SequenceModeling == 'BiLSTM':                                                             \n",
        "            self.SequenceModeling = nn.Sequential(                                                           \n",
        "                BidirectionalLSTM(self.FeatureExtraction_output, opt.hidden_size, opt.hidden_size),\n",
        "                BidirectionalLSTM(opt.hidden_size, opt.hidden_size, opt.hidden_size))\n",
        "            self.SequenceModeling_output = opt.hidden_size\n",
        "        else:\n",
        "            print('No SequenceModeling module specified')\n",
        "            self.SequenceModeling_output = self.FeatureExtraction_output\n",
        "\n",
        "        \"\"\" Prediction \"\"\"\n",
        "        if opt.Prediction == 'CTC':\n",
        "            self.Prediction = nn.Linear(self.SequenceModeling_output, opt.num_class)\n",
        "        elif opt.Prediction == 'Attn':\n",
        "            self.Prediction = Attention(self.SequenceModeling_output, opt.hidden_size, opt.num_class)\n",
        "        else:\n",
        "            raise Exception('Prediction is neither CTC or Attn')\n",
        "        \n",
        "        \"\"\" Classification Modeling\"\"\"                                                                   #added cls modeling \n",
        "        if opt.ClassificationModeling == 'FCN':\n",
        "            self.ClassificationModeling =  ClassificationNet(self.FeatureExtraction_output, opt.head1_nodes, opt.head2_nodes, opt.head3_nodes , label1, label2, label3) #if error-> nn.Sequential(flatten and then clsNet)\n",
        "        else:\n",
        "            print('Please specify ClassificationModeling module name as FCN')\n",
        "\n",
        "    def forward(self, input, text, is_train=True):                  #insert input parameters -->An array or 3 labels list + head nodes size \n",
        "        \"\"\" Transformation stage \"\"\"\n",
        "        if not self.stages['Trans'] == \"None\":\n",
        "            input = self.Transformation(input)\n",
        "\n",
        "        \"\"\" Feature extraction stage \"\"\"\n",
        "        visual_feature = self.FeatureExtraction(input)\n",
        "        visual_feature = self.AdaptiveAvgPool(visual_feature.permute(0, 3, 1, 2))  # [b, c, h, w] -> [b, w, c, h]\n",
        "        visual_feature = visual_feature.squeeze(3)\n",
        "                                                                                                                              \n",
        "        \"\"\" Sequence modeling stage \"\"\"\n",
        "        if self.stages['Seq'] == 'BiLSTM':\n",
        "            contextual_feature = self.SequenceModeling(visual_feature)\n",
        "        else:\n",
        "            contextual_feature = visual_feature  # for convenience. this is NOT contextually modeled by BiLSTM\n",
        "\n",
        "        \"\"\" Prediction stage \"\"\"\n",
        "        if self.stages['Pred'] == 'CTC':\n",
        "            prediction = self.Prediction(contextual_feature.contiguous())\n",
        "        else:\n",
        "            prediction = self.Prediction(contextual_feature.contiguous(), text, is_train, batch_max_length=self.opt.batch_max_length)\n",
        "                                                                                                                                        #added classification modeling stage\n",
        "        \"\"\" Classification Modeling stage \"\"\"\n",
        "        if self.stages['Cls'] == 'FCN':\n",
        "            pred1, pred2, pred3 = self.ClassificationModeling(visual_feature,)\n",
        "\n",
        "        return prediction, pred1, pred2, pred3  #(text chars, hsrp(standard or non-standard), languages (4-english, hindi, marathi, nepali), types of number plates - 7)"
      ],
      "metadata": {
        "id": "X_wZvg2OQmQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train"
      ],
      "metadata": {
        "id": "_1b-H5z0Q8KK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import random\n",
        "import string\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn.init as init\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import numpy as np\n",
        "\n",
        "from utils import CTCLabelConverter, CTCLabelConverterForBaiduWarpctc, AttnLabelConverter, Averager\n",
        "from dataset import hierarchical_dataset, AlignCollate, Batch_Balanced_Dataset\n",
        "from model import Model\n",
        "from test import validation\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def train(opt):\n",
        "    \"\"\" dataset preparation \"\"\"\n",
        "    if not opt.data_filtering_off:\n",
        "        print('Filtering the images containing characters which are not in opt.character')\n",
        "        print('Filtering the images whose label is longer than opt.batch_max_length')\n",
        "        # see https://github.com/clovaai/deep-text-recognition-benchmark/blob/6593928855fb7abb999a99f428b3e4477d4ae356/dataset.py#L130\n",
        "\n",
        "    opt.select_data = opt.select_data.split('-')\n",
        "    opt.batch_ratio = opt.batch_ratio.split('-')\n",
        "    train_dataset = Batch_Balanced_Dataset(opt)\n",
        "\n",
        "    log = open(f'./saved_models/{opt.exp_name}/log_dataset.txt', 'a')\n",
        "    AlignCollate_valid = AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=opt.PAD)\n",
        "    valid_dataset, valid_dataset_log = hierarchical_dataset(root=opt.valid_data, opt=opt)\n",
        "    valid_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset, batch_size=opt.batch_size,\n",
        "        shuffle=True,  # 'True' to check training progress with validation function.\n",
        "        num_workers=int(opt.workers),\n",
        "        collate_fn=AlignCollate_valid, pin_memory=True)\n",
        "    log.write(valid_dataset_log)\n",
        "    print('-' * 80)\n",
        "    log.write('-' * 80 + '\\n')\n",
        "    log.close()\n",
        "    \n",
        "    \"\"\" model configuration \"\"\"\n",
        "    if 'CTC' in opt.Prediction:\n",
        "        if opt.baiduCTC:\n",
        "            converter = CTCLabelConverterForBaiduWarpctc(opt.character)\n",
        "        else:\n",
        "            converter = CTCLabelConverter(opt.character)\n",
        "    else:\n",
        "        converter = AttnLabelConverter(opt.character)\n",
        "    opt.num_class = len(converter.character)\n",
        "\n",
        "    if opt.rgb:\n",
        "        opt.input_channel = 3\n",
        "    model = Model(opt)\n",
        "    print('model input parameters', opt.imgH, opt.imgW, opt.num_fiducial, opt.input_channel, opt.output_channel,\n",
        "          opt.hidden_size, opt.num_class, opt.batch_max_length, opt.head1_nodes, opt.head2_nodes, opt.head3_nodes, opt.Transformation, opt.FeatureExtraction,\n",
        "          opt.SequenceModeling, opt.Prediction, opt.ClassificationModeling)\n",
        "\n",
        "    # weight initialization\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'localization_fc2' in name:\n",
        "            print(f'Skip {name} as it is already initialized')\n",
        "            continue\n",
        "        try:\n",
        "            if 'bias' in name:\n",
        "                init.constant_(param, 0.0)\n",
        "            elif 'weight' in name:\n",
        "                init.kaiming_normal_(param)\n",
        "        except Exception as e:  # for batchnorm.\n",
        "            if 'weight' in name:\n",
        "                param.data.fill_(1)\n",
        "            continue\n",
        "\n",
        "    # data parallel for multi-GPU\n",
        "    model = torch.nn.DataParallel(model).to(device)\n",
        "    model.train()\n",
        "    if opt.saved_model != '':\n",
        "        print(f'loading pretrained model from {opt.saved_model}')\n",
        "        if opt.FT:\n",
        "            model.load_state_dict(torch.load(opt.saved_model), strict=False)\n",
        "        else:\n",
        "            model.load_state_dict(torch.load(opt.saved_model))\n",
        "    print(\"Model:\")\n",
        "    print(model)\n",
        "\n",
        "    \"\"\" setup loss \"\"\"                    #make it run everytime                         \n",
        "    if 'CTC' in opt.Prediction:                 #added FCN condition            and 'FCN' in opt.ClassificationModeling\n",
        "        if opt.baiduCTC:\n",
        "            # need to install warpctc. see our guideline.\n",
        "            from warpctc_pytorch import CTCLoss \n",
        "            criterion = CTCLoss()                            #-----------   #add 2 crterions in CTC --> cross entropy& binary and 1 in Attn--> binary ----wherever it's crterion--\n",
        "            criterion1 = torch.nn.BCELoss().to(device)                              #if err-> remove .to(device)\n",
        "            criterion2 = torch.nn.CrossEntropyLoss().to(device)                       #ignore index for [GO] token so, here it's not used\n",
        "        else:\n",
        "            criterion = torch.nn.CTCLoss(zero_infinity=True).to(device)\n",
        "            criterion1 = torch.nn.BCELoss().to(device)\n",
        "            criterion2 = torch.nn.CrossEntropyLoss().to(device)\n",
        "    else:\n",
        "        criterion = torch.nn.CrossEntropyLoss(ignore_index=0).to(device)  # ignore [GO] token = ignore index 0\n",
        "        criterion1 = torch.nn.BCELoss().to(device)\n",
        "        criterion2 = torch.nn.CrossEntropyLoss().to(device)    # for cls module\n",
        "    # loss averager\n",
        "    loss_avg = Averager()\n",
        "\n",
        "    # filter that only require gradient decent\n",
        "    filtered_parameters = []\n",
        "    params_num = []\n",
        "    for p in filter(lambda p: p.requires_grad, model.parameters()):\n",
        "        filtered_parameters.append(p)\n",
        "        params_num.append(np.prod(p.size()))\n",
        "    print('Trainable params num : ', sum(params_num))\n",
        "    # [print(name, p.numel()) for name, p in filter(lambda p: p[1].requires_grad, model.named_parameters())]\n",
        "\n",
        "    # setup optimizer\n",
        "    if opt.adam:\n",
        "        optimizer = optim.Adam(filtered_parameters, lr=opt.lr, betas=(opt.beta1, 0.999))\n",
        "    else:\n",
        "        optimizer = optim.Adadelta(filtered_parameters, lr=opt.lr, rho=opt.rho, eps=opt.eps)\n",
        "    print(\"Optimizer:\")\n",
        "    print(optimizer)\n",
        "\n",
        "    \"\"\" final options \"\"\"\n",
        "    # print(opt)\n",
        "    with open(f'./saved_models/{opt.exp_name}/opt.txt', 'a') as opt_file:\n",
        "        opt_log = '------------ Options -------------\\n'\n",
        "        args = vars(opt)\n",
        "        for k, v in args.items():\n",
        "            opt_log += f'{str(k)}: {str(v)}\\n'\n",
        "        opt_log += '---------------------------------------\\n'\n",
        "        print(opt_log)\n",
        "        opt_file.write(opt_log)\n",
        "\n",
        "    \"\"\" start training \"\"\"\n",
        "    start_iter = 0\n",
        "    if opt.saved_model != '':\n",
        "        try:\n",
        "            start_iter = int(opt.saved_model.split('_')[-1].split('.')[0])\n",
        "            print(f'continue to train, start_iter: {start_iter}')\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    start_time = time.time()\n",
        "    best_accuracy = -1\n",
        "    best_norm_ED = -1\n",
        "    iteration = start_iter\n",
        "\n",
        "    while(True):\n",
        "\n",
        "\n",
        "        # #changed#\n",
        "        # for i, (imgs, labels) in enumerate(dataloader):\n",
        "        #     imgs = imgs.to(device)\n",
        "        #     labels = [lbl.to(device) for lbl in labels] \n",
        "        # train part\n",
        "        image_tensors, labels = train_dataset.get_batch()                                #txt_lbl, lbl1, lbl2, lbl3\n",
        "        image = image_tensors.to(device)\n",
        "        text, length = converter.encode(labels, batch_max_length=opt.batch_max_length)\n",
        "        batch_size = image.size(0)\n",
        "\n",
        "\n",
        "        lambda1 = 0.9                                                        #added weights for tasks specific losses\n",
        "        lambda2 = 0.2\n",
        "        lambda3 = 0.4\n",
        "        lambda4 = 0.6\n",
        "        \n",
        "        if 'CTC' in opt.Prediction :                 #added FCN condition  and 'FCN' in opt.ClassificationModeling                                                      \n",
        "            preds = model(image, text)    #pd = model.....        ; model = Model(opt)\n",
        "            #preds, pred1, pred2, pred3 = pd                                                            \n",
        "            preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
        "            if opt.baiduCTC:\n",
        "                preds = preds.permute(1, 0, 2)  # to use CTCLoss format\n",
        "                cost = criterion(preds, text, preds_size, length) / batch_size\n",
        "                #adding cost1\n",
        "                #adding cost2\n",
        "                #adding cost3\n",
        "            else:\n",
        "                preds = preds.log_softmax(2).permute(1, 0, 2)\n",
        "                cost = criterion(preds, text, preds_size, length)\n",
        "                cost1 = #criterion1()\n",
        "                #adding cost2\n",
        "                #adding cost3\n",
        "\n",
        "        else:\n",
        "            preds = model(image, text[:, :-1])  # align with Attention.forward\n",
        "            target = text[:, 1:]  # without [GO] Symbol\n",
        "            cost = criterion(preds.view(-1, preds.shape[-1]), target.contiguous().view(-1))\n",
        "            #adding cost1\n",
        "            #adding cost2\n",
        "            #adding cost3\n",
        "        \n",
        "        #cost = lambda1*cost + lambda2*cost1 + lambda3*cost2 + lambda4*cost3\n",
        "        model.zero_grad()\n",
        "        cost.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), opt.grad_clip)  # gradient clipping with 5 (Default)\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_avg.add(cost)\n",
        "\n",
        "        # validation part\n",
        "        if (iteration + 1) % opt.valInterval == 0 or iteration == 0: # To see training progress, we also conduct validation when 'iteration == 0' \n",
        "            elapsed_time = time.time() - start_time\n",
        "            # for log\n",
        "            with open(f'./saved_models/{opt.exp_name}/log_train.txt', 'a') as log:\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    valid_loss, current_accuracy, current_norm_ED, preds, confidence_score, labels, infer_time, length_of_data = validation(\n",
        "                        model, criterion, valid_loader, converter, opt)\n",
        "                model.train()\n",
        "\n",
        "                # training loss and validation loss\n",
        "                loss_log = f'[{iteration+1}/{opt.num_iter}] Train loss: {loss_avg.val():0.5f}, Valid loss: {valid_loss:0.5f}, Elapsed_time: {elapsed_time:0.5f}'\n",
        "                loss_avg.reset()\n",
        "\n",
        "                current_model_log = f'{\"Current_accuracy\":17s}: {current_accuracy:0.3f}, {\"Current_norm_ED\":17s}: {current_norm_ED:0.2f}'\n",
        "\n",
        "                # keep best accuracy model (on valid dataset)\n",
        "                if current_accuracy > best_accuracy:\n",
        "                    best_accuracy = current_accuracy\n",
        "                    torch.save(model.state_dict(), f'./saved_models/{opt.exp_name}/best_accuracy.pth')\n",
        "                if current_norm_ED > best_norm_ED:\n",
        "                    best_norm_ED = current_norm_ED\n",
        "                    torch.save(model.state_dict(), f'./saved_models/{opt.exp_name}/best_norm_ED.pth')\n",
        "                best_model_log = f'{\"Best_accuracy\":17s}: {best_accuracy:0.3f}, {\"Best_norm_ED\":17s}: {best_norm_ED:0.2f}'\n",
        "\n",
        "                loss_model_log = f'{loss_log}\\n{current_model_log}\\n{best_model_log}'\n",
        "                print(loss_model_log)\n",
        "                log.write(loss_model_log + '\\n')\n",
        "\n",
        "                # show some predicted results\n",
        "                dashed_line = '-' * 80\n",
        "                head = f'{\"Ground Truth\":25s} | {\"Prediction\":25s} | Confidence Score & T/F'\n",
        "                predicted_result_log = f'{dashed_line}\\n{head}\\n{dashed_line}\\n'\n",
        "                for gt, pred, confidence in zip(labels[:5], preds[:5], confidence_score[:5]):\n",
        "                    if 'Attn' in opt.Prediction:\n",
        "                        gt = gt[:gt.find('[s]')]\n",
        "                        pred = pred[:pred.find('[s]')]\n",
        "\n",
        "                    predicted_result_log += f'{gt:25s} | {pred:25s} | {confidence:0.4f}\\t{str(pred == gt)}\\n'\n",
        "                predicted_result_log += f'{dashed_line}'\n",
        "                print(predicted_result_log)\n",
        "                log.write(predicted_result_log + '\\n')\n",
        "\n",
        "        # save model per 1e+5 iter.\n",
        "        if (iteration + 1) % 1e+5 == 0:\n",
        "            torch.save(\n",
        "                model.state_dict(), f'./saved_models/{opt.exp_name}/iter_{iteration+1}.pth')\n",
        "\n",
        "        if (iteration + 1) == opt.num_iter:\n",
        "            print('end the training')\n",
        "            sys.exit()\n",
        "        iteration += 1\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--exp_name', help='Where to store logs and models')\n",
        "    parser.add_argument('--train_data', required=True, help='path to training dataset')\n",
        "    parser.add_argument('--valid_data', required=True, help='path to validation dataset')\n",
        "    parser.add_argument('--manualSeed', type=int, default=1111, help='for random seed setting')\n",
        "    parser.add_argument('--workers', type=int, help='number of data loading workers', default=4)\n",
        "    parser.add_argument('--batch_size', type=int, default=192, help='input batch size')\n",
        "    parser.add_argument('--num_iter', type=int, default=300000, help='number of iterations to train for')\n",
        "    parser.add_argument('--valInterval', type=int, default=2000, help='Interval between each validation')\n",
        "    parser.add_argument('--saved_model', default='', help=\"path to model to continue training\")\n",
        "    parser.add_argument('--FT', action='store_true', help='whether to do fine-tuning')\n",
        "    parser.add_argument('--adam', action='store_true', help='Whether to use adam (default is Adadelta)')\n",
        "    parser.add_argument('--lr', type=float, default=1, help='learning rate, default=1.0 for Adadelta')\n",
        "    parser.add_argument('--beta1', type=float, default=0.9, help='beta1 for adam. default=0.9')\n",
        "    parser.add_argument('--rho', type=float, default=0.95, help='decay rate rho for Adadelta. default=0.95')\n",
        "    parser.add_argument('--eps', type=float, default=1e-8, help='eps for Adadelta. default=1e-8')\n",
        "    parser.add_argument('--grad_clip', type=float, default=5, help='gradient clipping value. default=5')\n",
        "    parser.add_argument('--baiduCTC', action='store_true', help='for data_filtering_off mode')\n",
        "    \"\"\" Data processing \"\"\"\n",
        "    parser.add_argument('--select_data', type=str, default='MJ-ST',\n",
        "                        help='select training data (default is MJ-ST, which means MJ and ST used as training data)')\n",
        "    parser.add_argument('--batch_ratio', type=str, default='0.5-0.5',\n",
        "                        help='assign ratio for each selected data in the batch')\n",
        "    parser.add_argument('--total_data_usage_ratio', type=str, default='1.0',\n",
        "                        help='total data usage ratio, this ratio is multiplied to total number of data.')\n",
        "    parser.add_argument('--batch_max_length', type=int, default=25, help='maximum-label-length')\n",
        "    parser.add_argument('--imgH', type=int, default=32, help='the height of the input image')\n",
        "    parser.add_argument('--imgW', type=int, default=100, help='the width of the input image')\n",
        "    parser.add_argument('--rgb', action='store_true', help='use rgb input')\n",
        "    parser.add_argument('--character', type=str,\n",
        "                        default='0123456789abcdefghijklmnopqrstuvwxyz', help='character label')\n",
        "    parser.add_argument('--sensitive', action='store_true', help='for sensitive character mode')\n",
        "    parser.add_argument('--PAD', action='store_true', help='whether to keep ratio then pad for image resize')\n",
        "    parser.add_argument('--data_filtering_off', action='store_true', help='for data_filtering_off mode')\n",
        "    \"\"\" Model Architecture \"\"\"\n",
        "    parser.add_argument('--Transformation', type=str, required=True, help='Transformation stage. None|TPS')\n",
        "    parser.add_argument('--FeatureExtraction', type=str, required=True,\n",
        "                        help='FeatureExtraction stage. VGG|RCNN|ResNet')\n",
        "    parser.add_argument('--SequenceModeling', type=str, required=True, help='SequenceModeling stage. None|BiLSTM')\n",
        "    parser.add_argument('--Prediction', type=str, required=True, help='Prediction stage. CTC|Attn')\n",
        "    parser.add_argument('--num_fiducial', type=int, default=20, help='number of fiducial points of TPS-STN')\n",
        "    parser.add_argument('--input_channel', type=int, default=1,\n",
        "                        help='the number of input channel of Feature extractor')\n",
        "    parser.add_argument('--output_channel', type=int, default=512,\n",
        "                        help='the number of output channel of Feature extractor')\n",
        "    parser.add_argument('--hidden_size', type=int, default=256, help='the size of the LSTM hidden state')\n",
        "\n",
        "    opt = parser.parse_args()\n",
        "\n",
        "    if not opt.exp_name:\n",
        "        opt.exp_name = f'{opt.Transformation}-{opt.FeatureExtraction}-{opt.SequenceModeling}-{opt.Prediction}'\n",
        "        opt.exp_name += f'-Seed{opt.manualSeed}'\n",
        "        # print(opt.exp_name)\n",
        "\n",
        "    os.makedirs(f'./saved_models/{opt.exp_name}', exist_ok=True)\n",
        "\n",
        "    \"\"\" vocab / character number configuration \"\"\"\n",
        "    if opt.sensitive:\n",
        "        # opt.character += 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
        "        opt.character = string.printable[:-6]  # same with ASTER setting (use 94 char).\n",
        "\n",
        "    \"\"\" Seed and GPU setting \"\"\"\n",
        "    # print(\"Random Seed: \", opt.manualSeed)\n",
        "    random.seed(opt.manualSeed)\n",
        "    np.random.seed(opt.manualSeed)\n",
        "    torch.manual_seed(opt.manualSeed)\n",
        "    torch.cuda.manual_seed(opt.manualSeed)\n",
        "\n",
        "    cudnn.benchmark = True\n",
        "    cudnn.deterministic = True\n",
        "    opt.num_gpu = torch.cuda.device_count()\n",
        "    # print('device count', opt.num_gpu)\n",
        "    if opt.num_gpu > 1:\n",
        "        print('------ Use multi-GPU setting ------')\n",
        "        print('if you stuck too long time with multi-GPU setting, try to set --workers 0')\n",
        "        # check multi-GPU issue https://github.com/clovaai/deep-text-recognition-benchmark/issues/1\n",
        "        opt.workers = opt.workers * opt.num_gpu\n",
        "        opt.batch_size = opt.batch_size * opt.num_gpu\n",
        "\n",
        "        \"\"\" previous version\n",
        "        print('To equlize batch stats to 1-GPU setting, the batch_size is multiplied with num_gpu and multiplied batch_size is ', opt.batch_size)\n",
        "        opt.batch_size = opt.batch_size * opt.num_gpu\n",
        "        print('To equalize the number of epochs to 1-GPU setting, num_iter is divided with num_gpu by default.')\n",
        "        If you dont care about it, just commnet out these line.)\n",
        "        opt.num_iter = int(opt.num_iter / opt.num_gpu)\n",
        "        \"\"\"\n",
        "\n",
        "    train(opt)"
      ],
      "metadata": {
        "id": "9gvu7CQEQ8VN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test"
      ],
      "metadata": {
        "id": "jF6uJ_XlQ8iE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import string\n",
        "import argparse\n",
        "import re\n",
        "\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from nltk.metrics.distance import edit_distance\n",
        "\n",
        "from utils import CTCLabelConverter, AttnLabelConverter, Averager\n",
        "from dataset import hierarchical_dataset, AlignCollate\n",
        "from model import Model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def benchmark_all_eval(model, criterion, converter, opt, calculate_infer_time=False):\n",
        "    \"\"\" evaluation with 10 benchmark evaluation datasets \"\"\"\n",
        "    # The evaluation datasets, dataset order is same with Table 1 in our paper.\n",
        "    eval_data_list = ['IIIT5k_3000', 'SVT', 'IC03_860', 'IC03_867', 'IC13_857',\n",
        "                      'IC13_1015', 'IC15_1811', 'IC15_2077', 'SVTP', 'CUTE80']\n",
        "\n",
        "    # # To easily compute the total accuracy of our paper.\n",
        "    # eval_data_list = ['IIIT5k_3000', 'SVT', 'IC03_867', \n",
        "    #                   'IC13_1015', 'IC15_2077', 'SVTP', 'CUTE80']\n",
        "\n",
        "    if calculate_infer_time:\n",
        "        evaluation_batch_size = 1  # batch_size should be 1 to calculate the GPU inference time per image.\n",
        "    else:\n",
        "        evaluation_batch_size = opt.batch_size\n",
        "\n",
        "    list_accuracy = []\n",
        "    total_forward_time = 0\n",
        "    total_evaluation_data_number = 0\n",
        "    total_correct_number = 0\n",
        "    log = open(f'./result/{opt.exp_name}/log_all_evaluation.txt', 'a')\n",
        "    dashed_line = '-' * 80\n",
        "    print(dashed_line)\n",
        "    log.write(dashed_line + '\\n')\n",
        "    for eval_data in eval_data_list:\n",
        "        eval_data_path = os.path.join(opt.eval_data, eval_data)\n",
        "        AlignCollate_evaluation = AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=opt.PAD)\n",
        "        eval_data, eval_data_log = hierarchical_dataset(root=eval_data_path, opt=opt)\n",
        "        evaluation_loader = torch.utils.data.DataLoader(\n",
        "            eval_data, batch_size=evaluation_batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=int(opt.workers),\n",
        "            collate_fn=AlignCollate_evaluation, pin_memory=True)\n",
        "\n",
        "        _, accuracy_by_best_model, norm_ED_by_best_model, _, _, _, infer_time, length_of_data = validation(\n",
        "            model, criterion, evaluation_loader, converter, opt)\n",
        "        list_accuracy.append(f'{accuracy_by_best_model:0.3f}')\n",
        "        total_forward_time += infer_time\n",
        "        total_evaluation_data_number += len(eval_data)\n",
        "        total_correct_number += accuracy_by_best_model * length_of_data\n",
        "        log.write(eval_data_log)\n",
        "        print(f'Acc {accuracy_by_best_model:0.3f}\\t normalized_ED {norm_ED_by_best_model:0.3f}')\n",
        "        log.write(f'Acc {accuracy_by_best_model:0.3f}\\t normalized_ED {norm_ED_by_best_model:0.3f}\\n')\n",
        "        print(dashed_line)\n",
        "        log.write(dashed_line + '\\n')\n",
        "\n",
        "    averaged_forward_time = total_forward_time / total_evaluation_data_number * 1000\n",
        "    total_accuracy = total_correct_number / total_evaluation_data_number\n",
        "    params_num = sum([np.prod(p.size()) for p in model.parameters()])\n",
        "\n",
        "    evaluation_log = 'accuracy: '\n",
        "    for name, accuracy in zip(eval_data_list, list_accuracy):\n",
        "        evaluation_log += f'{name}: {accuracy}\\t'\n",
        "    evaluation_log += f'total_accuracy: {total_accuracy:0.3f}\\t'\n",
        "    evaluation_log += f'averaged_infer_time: {averaged_forward_time:0.3f}\\t# parameters: {params_num/1e6:0.3f}'\n",
        "    print(evaluation_log)\n",
        "    log.write(evaluation_log + '\\n')\n",
        "    log.close()\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def validation(model, criterion, evaluation_loader, converter, opt):\n",
        "    \"\"\" validation or evaluation \"\"\"\n",
        "    n_correct = 0\n",
        "    norm_ED = 0\n",
        "    length_of_data = 0\n",
        "    infer_time = 0\n",
        "    valid_loss_avg = Averager()\n",
        "\n",
        "    for i, (image_tensors, labels) in enumerate(evaluation_loader):\n",
        "        batch_size = image_tensors.size(0)\n",
        "        length_of_data = length_of_data + batch_size\n",
        "        image = image_tensors.to(device)\n",
        "        # For max length prediction\n",
        "        length_for_pred = torch.IntTensor([opt.batch_max_length] * batch_size).to(device)\n",
        "        text_for_pred = torch.LongTensor(batch_size, opt.batch_max_length + 1).fill_(0).to(device)\n",
        "\n",
        "        text_for_loss, length_for_loss = converter.encode(labels, batch_max_length=opt.batch_max_length)\n",
        "\n",
        "        start_time = time.time()\n",
        "        if 'CTC' in opt.Prediction:\n",
        "            preds = model(image, text_for_pred)\n",
        "            forward_time = time.time() - start_time\n",
        "\n",
        "            # Calculate evaluation loss for CTC deocder.\n",
        "            preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
        "            # permute 'preds' to use CTCloss format\n",
        "            if opt.baiduCTC:\n",
        "                cost = criterion(preds.permute(1, 0, 2), text_for_loss, preds_size, length_for_loss) / batch_size\n",
        "            else:\n",
        "                cost = criterion(preds.log_softmax(2).permute(1, 0, 2), text_for_loss, preds_size, length_for_loss)\n",
        "\n",
        "            # Select max probabilty (greedy decoding) then decode index to character\n",
        "            if opt.baiduCTC:\n",
        "                _, preds_index = preds.max(2)\n",
        "                preds_index = preds_index.view(-1)\n",
        "            else:\n",
        "                _, preds_index = preds.max(2)\n",
        "            preds_str = converter.decode(preds_index.data, preds_size.data)\n",
        "        \n",
        "        else:\n",
        "            preds = model(image, text_for_pred, is_train=False)\n",
        "            forward_time = time.time() - start_time\n",
        "\n",
        "            preds = preds[:, :text_for_loss.shape[1] - 1, :]\n",
        "            target = text_for_loss[:, 1:]  # without [GO] Symbol\n",
        "            cost = criterion(preds.contiguous().view(-1, preds.shape[-1]), target.contiguous().view(-1))\n",
        "\n",
        "            # select max probabilty (greedy decoding) then decode index to character\n",
        "            _, preds_index = preds.max(2)\n",
        "            preds_str = converter.decode(preds_index, length_for_pred)\n",
        "            labels = converter.decode(text_for_loss[:, 1:], length_for_loss)\n",
        "\n",
        "        infer_time += forward_time\n",
        "        valid_loss_avg.add(cost)\n",
        "\n",
        "        # calculate accuracy & confidence score\n",
        "        preds_prob = F.softmax(preds, dim=2)\n",
        "        preds_max_prob, _ = preds_prob.max(dim=2)\n",
        "        confidence_score_list = []\n",
        "        for gt, pred, pred_max_prob in zip(labels, preds_str, preds_max_prob):\n",
        "            if 'Attn' in opt.Prediction:\n",
        "                gt = gt[:gt.find('[s]')]\n",
        "                pred_EOS = pred.find('[s]')\n",
        "                pred = pred[:pred_EOS]  # prune after \"end of sentence\" token ([s])\n",
        "                pred_max_prob = pred_max_prob[:pred_EOS]\n",
        "\n",
        "            # To evaluate 'case sensitive model' with alphanumeric and case insensitve setting.\n",
        "            if opt.sensitive and opt.data_filtering_off:\n",
        "                pred = pred.lower()\n",
        "                gt = gt.lower()\n",
        "                alphanumeric_case_insensitve = '0123456789abcdefghijklmnopqrstuvwxyz'\n",
        "                out_of_alphanumeric_case_insensitve = f'[^{alphanumeric_case_insensitve}]'\n",
        "                pred = re.sub(out_of_alphanumeric_case_insensitve, '', pred)\n",
        "                gt = re.sub(out_of_alphanumeric_case_insensitve, '', gt)\n",
        "\n",
        "            if pred == gt:\n",
        "                n_correct += 1\n",
        "\n",
        "            '''\n",
        "            (old version) ICDAR2017 DOST Normalized Edit Distance https://rrc.cvc.uab.es/?ch=7&com=tasks\n",
        "            \"For each word we calculate the normalized edit distance to the length of the ground truth transcription.\"\n",
        "            if len(gt) == 0:\n",
        "                norm_ED += 1\n",
        "            else:\n",
        "                norm_ED += edit_distance(pred, gt) / len(gt)\n",
        "            '''\n",
        "\n",
        "            # ICDAR2019 Normalized Edit Distance\n",
        "            if len(gt) == 0 or len(pred) == 0:\n",
        "                norm_ED += 0\n",
        "            elif len(gt) > len(pred):\n",
        "                norm_ED += 1 - edit_distance(pred, gt) / len(gt)\n",
        "            else:\n",
        "                norm_ED += 1 - edit_distance(pred, gt) / len(pred)\n",
        "\n",
        "            # calculate confidence score (= multiply of pred_max_prob)\n",
        "            try:\n",
        "                confidence_score = pred_max_prob.cumprod(dim=0)[-1]\n",
        "            except:\n",
        "                confidence_score = 0  # for empty pred case, when prune after \"end of sentence\" token ([s])\n",
        "            confidence_score_list.append(confidence_score)\n",
        "            # print(pred, gt, pred==gt, confidence_score)\n",
        "\n",
        "    accuracy = n_correct / float(length_of_data) * 100\n",
        "    norm_ED = norm_ED / float(length_of_data)  # ICDAR2019 Normalized Edit Distance\n",
        "\n",
        "    return valid_loss_avg.val(), accuracy, norm_ED, preds_str, confidence_score_list, labels, infer_time, length_of_data\n",
        "\n",
        "\n",
        "def test(opt):\n",
        "    \"\"\" model configuration \"\"\"\n",
        "    if 'CTC' in opt.Prediction:\n",
        "        converter = CTCLabelConverter(opt.character)\n",
        "    else:\n",
        "        converter = AttnLabelConverter(opt.character)\n",
        "    opt.num_class = len(converter.character)\n",
        "\n",
        "    if opt.rgb:\n",
        "        opt.input_channel = 3\n",
        "    model = Model(opt)\n",
        "    print('model input parameters', opt.imgH, opt.imgW, opt.num_fiducial, opt.input_channel, opt.output_channel,\n",
        "          opt.hidden_size, opt.num_class, opt.batch_max_length, opt.Transformation, opt.FeatureExtraction,\n",
        "          opt.SequenceModeling, opt.Prediction)\n",
        "    model = torch.nn.DataParallel(model).to(device)\n",
        "\n",
        "    # load model\n",
        "    print('loading pretrained model from %s' % opt.saved_model)\n",
        "    model.load_state_dict(torch.load(opt.saved_model, map_location=device))\n",
        "    opt.exp_name = '_'.join(opt.saved_model.split('/')[1:])\n",
        "    # print(model)\n",
        "\n",
        "    \"\"\" keep evaluation model and result logs \"\"\"\n",
        "    os.makedirs(f'./result/{opt.exp_name}', exist_ok=True)\n",
        "    os.system(f'cp {opt.saved_model} ./result/{opt.exp_name}/')\n",
        "\n",
        "    \"\"\" setup loss \"\"\"\n",
        "    if 'CTC' in opt.Prediction:\n",
        "        criterion = torch.nn.CTCLoss(zero_infinity=True).to(device)\n",
        "    else:\n",
        "        criterion = torch.nn.CrossEntropyLoss(ignore_index=0).to(device)  # ignore [GO] token = ignore index 0\n",
        "\n",
        "    \"\"\" evaluation \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        if opt.benchmark_all_eval:  # evaluation with 10 benchmark evaluation datasets\n",
        "            benchmark_all_eval(model, criterion, converter, opt)\n",
        "        else:\n",
        "            log = open(f'./result/{opt.exp_name}/log_evaluation.txt', 'a')\n",
        "            AlignCollate_evaluation = AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=opt.PAD)\n",
        "            eval_data, eval_data_log = hierarchical_dataset(root=opt.eval_data, opt=opt)\n",
        "            evaluation_loader = torch.utils.data.DataLoader(\n",
        "                eval_data, batch_size=opt.batch_size,\n",
        "                shuffle=False,\n",
        "                num_workers=int(opt.workers),\n",
        "                collate_fn=AlignCollate_evaluation, pin_memory=True)\n",
        "            _, accuracy_by_best_model, _, _, _, _, _, _ = validation(\n",
        "                model, criterion, evaluation_loader, converter, opt)\n",
        "            log.write(eval_data_log)\n",
        "            print(f'{accuracy_by_best_model:0.3f}')\n",
        "            log.write(f'{accuracy_by_best_model:0.3f}\\n')\n",
        "            log.close()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--eval_data', required=True, help='path to evaluation dataset')\n",
        "    parser.add_argument('--benchmark_all_eval', action='store_true', help='evaluate 10 benchmark evaluation datasets')\n",
        "    parser.add_argument('--workers', type=int, help='number of data loading workers', default=4)\n",
        "    parser.add_argument('--batch_size', type=int, default=192, help='input batch size')\n",
        "    parser.add_argument('--saved_model', required=True, help=\"path to saved_model to evaluation\")\n",
        "    \"\"\" Data processing \"\"\"\n",
        "    parser.add_argument('--batch_max_length', type=int, default=25, help='maximum-label-length')\n",
        "    parser.add_argument('--imgH', type=int, default=32, help='the height of the input image')\n",
        "    parser.add_argument('--imgW', type=int, default=100, help='the width of the input image')\n",
        "    parser.add_argument('--rgb', action='store_true', help='use rgb input')\n",
        "    parser.add_argument('--character', type=str, default='0123456789abcdefghijklmnopqrstuvwxyz', help='character label')\n",
        "    parser.add_argument('--sensitive', action='store_true', help='for sensitive character mode')\n",
        "    parser.add_argument('--PAD', action='store_true', help='whether to keep ratio then pad for image resize')\n",
        "    parser.add_argument('--data_filtering_off', action='store_true', help='for data_filtering_off mode')\n",
        "    parser.add_argument('--baiduCTC', action='store_true', help='for data_filtering_off mode')\n",
        "    \"\"\" Model Architecture \"\"\"\n",
        "    parser.add_argument('--Transformation', type=str, required=True, help='Transformation stage. None|TPS')\n",
        "    parser.add_argument('--FeatureExtraction', type=str, required=True, help='FeatureExtraction stage. VGG|RCNN|ResNet')\n",
        "    parser.add_argument('--SequenceModeling', type=str, required=True, help='SequenceModeling stage. None|BiLSTM')\n",
        "    parser.add_argument('--Prediction', type=str, required=True, help='Prediction stage. CTC|Attn')\n",
        "    parser.add_argument('--num_fiducial', type=int, default=20, help='number of fiducial points of TPS-STN')\n",
        "    parser.add_argument('--input_channel', type=int, default=1, help='the number of input channel of Feature extractor')\n",
        "    parser.add_argument('--output_channel', type=int, default=512,\n",
        "                        help='the number of output channel of Feature extractor')\n",
        "    parser.add_argument('--hidden_size', type=int, default=256, help='the size of the LSTM hidden state')\n",
        "\n",
        "    opt = parser.parse_args()\n",
        "\n",
        "    \"\"\" vocab / character number configuration \"\"\"\n",
        "    if opt.sensitive:\n",
        "        opt.character = string.printable[:-6]  # same with ASTER setting (use 94 char).\n",
        "\n",
        "    cudnn.benchmark = True\n",
        "    cudnn.deterministic = True\n",
        "    opt.num_gpu = torch.cuda.device_count()\n",
        "\n",
        "    test(opt)"
      ],
      "metadata": {
        "id": "avEanGwXQ8ti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Utils (for sequencial modeling)"
      ],
      "metadata": {
        "id": "V6psOpuoRbAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "class CTCLabelConverter(object):\n",
        "    \"\"\" Convert between text-label and text-index \"\"\"\n",
        "\n",
        "    def __init__(self, character):\n",
        "        # character (str): set of the possible characters.\n",
        "        dict_character = list(character)\n",
        "\n",
        "        self.dict = {}\n",
        "        for i, char in enumerate(dict_character):\n",
        "            # NOTE: 0 is reserved for 'CTCblank' token required by CTCLoss\n",
        "            self.dict[char] = i + 1\n",
        "\n",
        "        self.character = ['[CTCblank]'] + dict_character  # dummy '[CTCblank]' token for CTCLoss (index 0)\n",
        "\n",
        "    def encode(self, text, batch_max_length=25):\n",
        "        \"\"\"convert text-label into text-index.\n",
        "        input:\n",
        "            text: text labels of each image. [batch_size]\n",
        "            batch_max_length: max length of text label in the batch. 25 by default\n",
        "        output:\n",
        "            text: text index for CTCLoss. [batch_size, batch_max_length]\n",
        "            length: length of each text. [batch_size]\n",
        "        \"\"\"\n",
        "        length = [len(s) for s in text]\n",
        "\n",
        "        # The index used for padding (=0) would not affect the CTC loss calculation.\n",
        "        batch_text = torch.LongTensor(len(text), batch_max_length).fill_(0)\n",
        "        for i, t in enumerate(text):\n",
        "            text = list(t)\n",
        "            text = [self.dict[char] for char in text]\n",
        "            batch_text[i][:len(text)] = torch.LongTensor(text)\n",
        "        return (batch_text.to(device), torch.IntTensor(length).to(device))\n",
        "\n",
        "    def decode(self, text_index, length):\n",
        "        \"\"\" convert text-index into text-label. \"\"\"\n",
        "        texts = []\n",
        "        for index, l in enumerate(length):\n",
        "            t = text_index[index, :]\n",
        "\n",
        "            char_list = []\n",
        "            for i in range(l):\n",
        "                if t[i] != 0 and (not (i > 0 and t[i - 1] == t[i])):  # removing repeated characters and blank.\n",
        "                    char_list.append(self.character[t[i]])\n",
        "            text = ''.join(char_list)\n",
        "\n",
        "            texts.append(text)\n",
        "        return texts\n",
        "\n",
        "\n",
        "class CTCLabelConverterForBaiduWarpctc(object):\n",
        "    \"\"\" Convert between text-label and text-index for baidu warpctc \"\"\"\n",
        "\n",
        "    def __init__(self, character):\n",
        "        # character (str): set of the possible characters.\n",
        "        dict_character = list(character)\n",
        "\n",
        "        self.dict = {}\n",
        "        for i, char in enumerate(dict_character):\n",
        "            # NOTE: 0 is reserved for 'CTCblank' token required by CTCLoss\n",
        "            self.dict[char] = i + 1\n",
        "\n",
        "        self.character = ['[CTCblank]'] + dict_character  # dummy '[CTCblank]' token for CTCLoss (index 0)\n",
        "\n",
        "    def encode(self, text, batch_max_length=25):\n",
        "        \"\"\"convert text-label into text-index.\n",
        "        input:\n",
        "            text: text labels of each image. [batch_size]\n",
        "        output:\n",
        "            text: concatenated text index for CTCLoss.\n",
        "                    [sum(text_lengths)] = [text_index_0 + text_index_1 + ... + text_index_(n - 1)]\n",
        "            length: length of each text. [batch_size]\n",
        "        \"\"\"\n",
        "        length = [len(s) for s in text]\n",
        "        text = ''.join(text)\n",
        "        text = [self.dict[char] for char in text]\n",
        "\n",
        "        return (torch.IntTensor(text), torch.IntTensor(length))\n",
        "\n",
        "    def decode(self, text_index, length):\n",
        "        \"\"\" convert text-index into text-label. \"\"\"\n",
        "        texts = []\n",
        "        index = 0\n",
        "        for l in length:\n",
        "            t = text_index[index:index + l]\n",
        "\n",
        "            char_list = []\n",
        "            for i in range(l):\n",
        "                if t[i] != 0 and (not (i > 0 and t[i - 1] == t[i])):  # removing repeated characters and blank.\n",
        "                    char_list.append(self.character[t[i]])\n",
        "            text = ''.join(char_list)\n",
        "\n",
        "            texts.append(text)\n",
        "            index += l\n",
        "        return texts\n",
        "\n",
        "\n",
        "class AttnLabelConverter(object):\n",
        "    \"\"\" Convert between text-label and text-index \"\"\"\n",
        "\n",
        "    def __init__(self, character):\n",
        "        # character (str): set of the possible characters.\n",
        "        # [GO] for the start token of the attention decoder. [s] for end-of-sentence token.\n",
        "        list_token = ['[GO]', '[s]']  # ['[s]','[UNK]','[PAD]','[GO]']\n",
        "        list_character = list(character)\n",
        "        self.character = list_token + list_character\n",
        "\n",
        "        self.dict = {}\n",
        "        for i, char in enumerate(self.character):\n",
        "            # print(i, char)\n",
        "            self.dict[char] = i\n",
        "\n",
        "    def encode(self, text, batch_max_length=25):\n",
        "        \"\"\" convert text-label into text-index.\n",
        "        input:\n",
        "            text: text labels of each image. [batch_size]\n",
        "            batch_max_length: max length of text label in the batch. 25 by default\n",
        "        output:\n",
        "            text : the input of attention decoder. [batch_size x (max_length+2)] +1 for [GO] token and +1 for [s] token.\n",
        "                text[:, 0] is [GO] token and text is padded with [GO] token after [s] token.\n",
        "            length : the length of output of attention decoder, which count [s] token also. [3, 7, ....] [batch_size]\n",
        "        \"\"\"\n",
        "        length = [len(s) + 1 for s in text]  # +1 for [s] at end of sentence.\n",
        "        # batch_max_length = max(length) # this is not allowed for multi-gpu setting\n",
        "        batch_max_length += 1\n",
        "        # additional +1 for [GO] at first step. batch_text is padded with [GO] token after [s] token.\n",
        "        batch_text = torch.LongTensor(len(text), batch_max_length + 1).fill_(0)\n",
        "        for i, t in enumerate(text):\n",
        "            text = list(t)\n",
        "            text.append('[s]')\n",
        "            text = [self.dict[char] for char in text]\n",
        "            batch_text[i][1:1 + len(text)] = torch.LongTensor(text)  # batch_text[:, 0] = [GO] token\n",
        "        return (batch_text.to(device), torch.IntTensor(length).to(device))\n",
        "\n",
        "    def decode(self, text_index, length):\n",
        "        \"\"\" convert text-index into text-label. \"\"\"\n",
        "        texts = []\n",
        "        for index, l in enumerate(length):\n",
        "            text = ''.join([self.character[i] for i in text_index[index, :]])\n",
        "            texts.append(text)\n",
        "        return texts\n",
        "\n",
        "\n",
        "class Averager(object):\n",
        "    \"\"\"Compute average for torch.Tensor, used for loss average.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def add(self, v):\n",
        "        count = v.data.numel()\n",
        "        v = v.data.sum()\n",
        "        self.n_count += count\n",
        "        self.sum += v\n",
        "\n",
        "    def reset(self):\n",
        "        self.n_count = 0\n",
        "        self.sum = 0\n",
        "\n",
        "    def val(self):\n",
        "        res = 0\n",
        "        if self.n_count != 0:\n",
        "            res = self.sum / float(self.n_count)\n",
        "        return res"
      ],
      "metadata": {
        "id": "RS5Qiu_ZRbLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Code ref link](https://github.com/Coderx7/A-Quick-and-Simple-Pytorch-Tutorial/blob/master/MultiTaskLearning.py)"
      ],
      "metadata": {
        "id": "exo8IyErUAe7"
      }
    }
  ]
}